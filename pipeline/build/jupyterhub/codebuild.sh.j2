set -ex

#######
#  When calling script don't forget to prepend all the ingested environment variables
#
#    - AWS_AccountId=${AWS::AccountId} 
#       AWS_Region=${AWS::Region} 
#       ContainerNamespace=${ContainerNamespace} 
#       CostTagKey=${CostTagKey} 
#       CostTagValue=${CostTagValue} 
#       LabShortName=${LabShortName} 
#       PortalDomain=${PortalDomain} 
#       AdminUserName=${AdminUserName} 
#       NodeProxyPort=${NodeProxyPort} 
#       AZPostfix=${AZPostfix} 
#       DaysTillVolumeDeletion=${DaysTillVolumeDeletion} 
#       DaysTillSnapshotDeletion=${DaysTillSnapshotDeletion} 
#       CODEBUILD_ROOT=$CODEBUILD_ROOT
#       KubectlVersion=${KubectlVersion}
#       Helm3Version=${Helm3Version}
#       AWSEbsCsiDriverVersion=${AWSEbsCsiDriverVersion}
#       JupyterHubHelmVersion=${JupyterHubHelmVersion}
#       AWSK8sCNIVersion=${AWSK8sCNIVersion}
#       ClusterAutoscalerHelmVersion=${ClusterAutoscalerHelmVersion}
#       JupyterHubImageVersion=${JupyterHubImageVersion}
#      bash codebuild.sh
#
#######

####### ******************
# Versions of software that need to be updated every cluster update
printf "\n\n%s\n" "******* ENV Variables (not including secrets):"
printf "%s\n" "
AWS_AccountId=${AWS_AccountId} 
AWS_Region=${AWS_Region} 
ContainerNamespace=${ContainerNamespace} 
CostTagKey=${CostTagKey} 
CostTagValue=${CostTagValue} 
LabShortName=${LabShortName} 
PortalDomain=${PortalDomain} 
AdminUserName=${AdminUserName} 
NodeProxyPort=${NodeProxyPort} 
AZPostfix=${AZPostfix} 
DaysTillVolumeDeletion=${DaysTillVolumeDeletion} 
DaysTillSnapshotDeletion=${DaysTillSnapshotDeletion} 
CODEBUILD_ROOT=${CODEBUILD_ROOT}
KubectlVersion=${KubectlVersion}
Helm3Version=${Helm3Version}
AWSEbsCsiDriverVersion=${AWSEbsCsiDriverVersion}
JupyterHubHelmVersion=${JupyterHubHelmVersion}
AWSK8sCNIVersion=${AWSK8sCNIVersion}
ClusterAutoscalerHelmVersion=${ClusterAutoscalerHelmVersion}
JupyterHubImageVersion=${JupyterHubImageVersion}
"

####### ******************
# Install

pip3 install boto3 --upgrade
pip3 install kubernetes --upgrade
pip3 install jinja2 --upgrade

curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/$KubectlVersion/bin/darwin/amd64/kubectl

curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash -s -- --version $Helm3Version

export HELM_HOST=127.0.0.1:44134
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp;
mv /tmp/eksctl /usr/local/bin;
eksctl version;


####### ******************
# Prebuild

printf "\n\n%s\n" "******* IAM build role used is... $(aws sts get-caller-identity)"

printf "\n\n%s\n" "******* Logging into AWS ECR...";
aws ecr get-login-password --region ${AWS_Region} | \
    docker login --username AWS --password-stdin ${AWS_AccountId}.dkr.ecr.${AWS_Region}.amazonaws.com

printf "\n\n%s\n" "******* Install helm charts and update...";
helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/;
helm repo add autoscaler https://kubernetes.github.io/autoscaler;
helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver;
helm repo update;


####### *******************
# Build

printf "\n\n%s\n" "******* Get registry URI..";
export REGISTRY_URI=${AWS_AccountId}.dkr.ecr.${AWS_Region}.amazonaws.com/${ContainerNamespace}
printf "%s\n" "REGISTRY_URI: $REGISTRY_URI"

#######
printf "\n\n%s\n" "******* Update assumed role of cluster-build...";
cd ${CODEBUILD_ROOT}/pipeline/configs/;

cp cluster-build-assumerolepolicy.template.json assume.json;
sed -i "s|CLUSTER_RUN_ARN|arn:aws:iam::${AWS_AccountId}:role/${AWS_Region}-${CostTagValue}-cluster-run-role|" assume.json;
aws iam update-assume-role-policy --role-name ${AWS_Region}-${CostTagValue}-cluster-build-role --policy-document file://assume.json;
sleep 10

#######
printf "\n\n%s\n" "******* Update kubeconfig and apply config files...";
aws eks update-kubeconfig --name ${CostTagValue}-cluster --role-arn arn:aws:iam::${AWS_AccountId}:role/${AWS_Region}-${CostTagValue}-cluster-build-role;

#######
printf "\n\n%s\n" "******* Apply eks console bindings and aws-auth.yaml...";
kubectl apply -f https://s3.us-west-2.amazonaws.com/amazon-eks/docs/eks-console-full-access.yaml
kubectl apply -f aws-auth-cm.yaml

#######
printf "\n\n%s\n" "******* Reapply storage class...";
kubectl delete sc gp2 || true;
kubectl apply -f csi-sc.yaml

#######
printf "\n\n%s\n" "******* Apply ebs csi driver...";
# if there is an error, then `kubectl delete csidriver ebs.csi.aws.com` and reapply. Warning: This will break volume management until reran.
helm upgrade aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
    --install \
    --version $AWSEbsCsiDriverVersion \
    --namespace kube-system \
    --timeout=6m0s \
    --atomic \
    --set controller.extraCreateMetadata=true \
    --set controller.k8sTagClusterId=${CostTagValue}-cluster \
    --set controller.extraVolumeTags.${CostTagKey}=${CostTagValue}

#######
printf "\n\n%s\n" "******* Render Service Accounts...";
cd ${CODEBUILD_ROOT}/pipeline/build/jupyterhub/;
python3 service_accounts.py

#######
printf "\n\n%s\n" "******* Build and deploy custom Cron services...";
cd ${CODEBUILD_ROOT}/services/crons

cp dockerfile dockerfile.build;
export CRONS_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
time docker build -f dockerfile.build -t $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD -t $REGISTRY_URI/crons:latest .;
docker push $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD;
docker push $REGISTRY_URI/crons:latest;

# Secret applied later
sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/crons:$CRONS_IMAGE_BUILD|" k8s/crons.yaml;
kubectl apply -f k8s/crons.yaml;

#######
printf "\n\n%s\n" "******* Apply k8s resources for services...";
cd ${CODEBUILD_ROOT}/services
kubectl apply -f k8s.yaml

#######
printf "\n\n%s\n" "******* Build JupyterHub Image...";
cd ${CODEBUILD_ROOT}/jupyterhub/;

cp dockerfile dockerfile.build;
export HUB_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
time docker build --build-arg JUPYTERHUB_HUB_HELM_VERSION=${JupyterHubHelmVersion} -f dockerfile.build -t $REGISTRY_URI/hub:$HUB_IMAGE_BUILD -t $REGISTRY_URI/hub:latest .;
docker push $REGISTRY_URI/hub:$HUB_IMAGE_BUILD;
docker push $REGISTRY_URI/hub:latest;

#######
printf "\n\n%s\n" "******* Various cluster env variables...";
printf "%s\n" "REGISTRY_URI $REGISTRY_URI";
printf "%s\n" "HUB_IMAGE_BUILD $HUB_IMAGE_BUILD";

#######
printf "\n\n%s\n" "******* Install JupyterHub cluster...";
cd ${CODEBUILD_ROOT}/jupyterhub/;
helm upgrade jupyter jupyterhub/jupyterhub \
    --install \
    --create-namespace \
    --namespace jupyter \
    --version $JupyterHubHelmVersion \
    --values helm_config.yaml \
    --timeout=6m0s \
    --atomic \
    --set hub.image.name=$REGISTRY_URI/hub \
    --set hub.image.tag=$HUB_IMAGE_BUILD \
    --set hub.baseUrl="/lab/${LabShortName}/" \
    --set hub.extraEnv.JUPYTERHUB_LAB_NAME="${LabShortName}" \
    --set hub.extraEnv.OPENSCIENCELAB_PORTAL_DOMAIN="${PortalDomain}" \
    --set hub.config.Authenticator.admin_users[0]=${AdminUserName} \
    --set proxy.service.nodePorts.http=${NodeProxyPort} \
    --set custom.REGISTRY_URI=$REGISTRY_URI \
    --set custom.CLUSTER_NAME="${CostTagValue}-cluster" \
    --set custom.AZ_NAME="${AWS_Region}${AZPostfix}" \
    --set custom.AWS_REGION="${AWS_REGION}" \
    --set custom.COST_TAG_VALUE="${CostTagValue}" \
    --set custom.COST_TAG_KEY="${CostTagKey}" \
    --set custom.DAYS_TILL_VOLUME_DELETION="${DaysTillVolumeDeletion}" \
    --set custom.DAYS_TILL_SNAPSHOT_DELETION="${DaysTillSnapshotDeletion}" \
    --set-file singleuser.extraFiles.user-hooks-pull.stringData='./singleuser/hooks/etc/pull.py' \
    --set-file singleuser.extraFiles.user-hooks-clean.stringData='./singleuser/hooks/etc/pkg_clean.py' \
    --set-file singleuser.extraFiles.user-hooks-kernel-flag.stringData='./singleuser/hooks/etc/old_kernels_flag.txt' \
    --set-file singleuser.extraFiles.user-hooks-kernel-flag-readme.stringData='./singleuser/hooks/etc/kernels_rename_README' \
    --set-file singleuser.extraFiles.user-others-check_storage.stringData='./singleuser/others/check_storage.py' \
    --set-file singleuser.extraFiles.user-custom_magics.stringData='./singleuser/custom_magics/00-df.py' \
    --set-file singleuser.extraFiles.user-extensions-overrides.stringData='./singleuser/extensions/overrides/overrides.json' \
    --set-file singleuser.extraFiles.user-template-page.stringData='./singleuser/templates/page.html' \
    --set-file singleuser.extraFiles.user-template-tree.stringData='./singleuser/templates/tree.html' \
    {% for file_name in hook_script_filenames -%}
    --set-file singleuser.extraFiles.user-hooks-{{ file_name | replace('.sh', '') }}.stringData='./singleuser/hooks/{{ file_name }}' \
    {% endfor -%}
    {% for file_name in override_script_filenames -%}
    --set-file singleuser.extraFiles.user-overrides-{{ file_name | replace('.json', '') }}.stringData='./singleuser/overrides/{{ file_name }}' \
    {% endfor -%}
    --

kubectl -n jupyter rollout status -w deployment.apps/hub

printf "\n\n%s\n" "******* Associate volume provisioner with service account...";
kubectl create clusterrolebinding cluster-pv --clusterrole=system:persistent-volume-provisioner --serviceaccount=jupyter:hub --dry-run=true -o yaml | kubectl apply -f -;

printf "\n\n%s\n" "******* Apply CNI. This must be updated on every cluster upgrade...";
curl -o aws-k8s-cni.yaml https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/$AWSK8sCNIVersion/config/master/aws-k8s-cni.yaml;

sed -i "s/us-west-2/${AWS_Region}/" aws-k8s-cni.yaml;
kubectl apply -f aws-k8s-cni.yaml

#######
printf "\n\n%s\n" "******* Install autoscaler...";
helm upgrade autoscaler autoscaler/cluster-autoscaler \
    --install \
    --version $ClusterAutoscalerHelmVersion \
    --create-namespace \
    --namespace autoscaler \
    --atomic \
    --timeout=2m0s \
    --set autoDiscovery.clusterName=${CostTagValue}-cluster \
    --set awsRegion=${AWS_Region} \
    --set nodeSelector."hub\\.jupyter\\.org/node-purpose"=core


#######
printf "\n\n%s\n" "******* Install Fluent Bit for AWS Container Insights...";

export FluentBitHttpPort='2020';
export FluentBitReadFromHead='Off';

[[ $FluentBitReadFromHead = 'On' ]] && FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On';
[[ -z $FluentBitHttpPort ]] && FluentBitHttpServer='Off' || FluentBitHttpServer='On';

curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml > cwagent-fluent-bit-quickstart.yaml;
sed -i "s|{% raw %}{{cluster_name}}{% endraw %}|${CostTagValue}-cluster|" cwagent-fluent-bit-quickstart.yaml;
sed -i "s|{% raw %}{{region_name}}{% endraw %}|${AWS_Region}|" cwagent-fluent-bit-quickstart.yaml;
sed -i "s|{% raw %}{{http_server_toggle}}{% endraw %}|\"$FluentBitHttpServer\"|" cwagent-fluent-bit-quickstart.yaml;
sed -i "s|{% raw %}{{http_server_port}}{% endraw %}|\"$FluentBitHttpPort\"|" cwagent-fluent-bit-quickstart.yaml;
sed -i "s|{% raw %}{{read_from_head}}{% endraw %}|\"$FluentBitReadFromHead\"|" cwagent-fluent-bit-quickstart.yaml;
sed -i "s|{% raw %}{{read_from_tail}}{% endraw %}|\"$FluentBitReadFromTail\"|" cwagent-fluent-bit-quickstart.yaml;
kubectl apply -f cwagent-fluent-bit-quickstart.yaml;

#######
printf "\n\n%s\n" "******* Install Prometheus for CloudWatch Insights";
cd ${CODEBUILD_ROOT}/pipeline/configs/;

sed -i "s|{% raw %}{{cluster_name}}{% endraw %}|${CostTagValue}-cluster|; s|{% raw %}{{region_name}}{% endraw %}|${AWS_Region}|" cwagent-prometheus.yaml;

kubectl apply -f cwagent-prometheus.yaml


printf "\n\n%s\n" "The End"

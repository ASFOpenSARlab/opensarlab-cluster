---

{% set parameters = opensciencelab['parameters'] -%}
{% set nodes = opensciencelab['nodes'] -%}
{% set profiles = opensciencelab['profiles'] -%}

AWSTemplateFormatVersion: 2010-09-09

Parameters:
  CertificateArn:
    Description: The ARN of the SSL certificate attached to the load balancer.
    Type: String
    Default: {{ parameters.certificate_arn }}

  ContainerNamespace:
    Description: The full url of the container ECR. Example-- osl-e-dev-container
    Type: String
    Default: {{ parameters.container_namespace }}

  CostTagKey:
    Type: String
    Description: "The name of the cost allocation tag you set up in AWS to track deployment costs, i.e. deployment_name"
    Default: {{ parameters.cost_tag_key }}

  CostTagValue:
    Type: String
    Description: "The value of the cost tag used for filtering the budget, etc."
    Default: {{ parameters.cost_tag_value }}

  # The following are dynamically added by the pipeline
  VpcId:
    Description: The VPC of the worker instances
    Type: AWS::EC2::VPC::Id
    Default: {{ parameters.vpc_id }}

  Subnets:
    Description: The subnets where workers can be created.
    Type: List<AWS::EC2::Subnet::Id>
    Default: {{ parameters.all_subnets }}

  ActiveSubnets:
    Description: The subnets actually used by resources in the cluster. Typically only one (e.g., subnet for AZ -d).
    Type: List<AWS::EC2::Subnet::Id>
    Default: {{ parameters.active_subnets }}

  # The following values require defaults.
  LoadBalancerCidrBlock:
    Description: The range of allowed IPv4 addresses for the load balancer. This only firewalls the load balancer URL and not the cluster in general.
    Type: String
    Default: 0.0.0.0/0

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

  NodeImageId:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/{{ parameters.eks_version }}/amazon-linux-2/recommended/image_id

  # Software versions
  EksVersion:
    Description: The version of the EKS Cluster. 
    Type: String
    Default: {{ parameters.eks_version }}

  # Squid stuff
  SquidAmiId:
    Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
    Default: '/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64'
    Description: AMI ID pointer in AWS Systems Manager Parameter Store. Default value points to the
      latest Amazon Linux 2 AMI ID.

  SquidInstanceType:
    Type: String
    Default: t3.small
    Description: Instance type to use to launch the NAT instances.
    AllowedValues:
      - t3.nano
      - t3.micro
      - t3.small
      - t3.medium
      - t3.large
      - m4.large
      - m4.xlarge
      - m4.2xlarge
      - m5.large
      - m5.xlarge
      - m5.2xlarge
      - c4.large
      - c4.xlarge
      - c4.large
      - c5.large
      - c5.xlarge
      - c5.large

Outputs:
  AppUrl:
    Value: !GetAtt LoadBalancer.DNSName

Resources:

  ClusterRunRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-run-role
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - eks.amazonaws.com
                - codebuild.amazonaws.com
                - cloudformation.amazonaws.com
                - codepipeline.amazonaws.com
                - iam.amazonaws.com
                - ec2.amazonaws.com
            Action: sts:AssumeRole
          - Effect: Allow
            Principal:
              AWS: !Sub arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-build-role
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
        - arn:aws:iam::aws:policy/AmazonEKSServicePolicy
        - arn:aws:iam::aws:policy/AutoScalingFullAccess
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${CostTagValue}-cluster-run-extra-policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "codestar-connections:*"
                  - "s3:*"
                  - "dlm:*"
                  - "logs:*"
                  - "cloudformation:*"
                  - "elasticloadbalancing:*"
                  - "autoscaling:*"
                  - "codebuild:*"
                  - "iam:*"
                  - "secretsmanager:*"
                  - "ssm:*"
                  - "ecr:*"
                  - "ec2:*"
                  - "eks:*"
                  - "sts:AssumeRole"
                Resource: '*'

  Cluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name: !Sub ${CostTagValue}-cluster
      RoleArn: !Sub arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-run-role
      ResourcesVpcConfig:
        SubnetIds: !Ref Subnets
        SecurityGroupIds:
          - !GetAtt ClusterSecurityGroup.GroupId
      Version: !Sub ${EksVersion}

  ClusterSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${CostTagValue}-cluster
      GroupDescription: !Sub Security group for the ${CostTagValue}-cluster EKS cluster
      VpcId: !Ref VpcId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  ClusterUserAccessFullGroup:
    Type: AWS::IAM::Group
    Properties:
      GroupName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-access-full-group
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-access-full-group
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - "sts:AssumeRole"
                Resource: !GetAtt ClusterUserAccessFullRole.Arn

  ClusterUserAccessFullRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-full-access
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub arn:aws:iam::${AWS::AccountId}:root
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-full-access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "eks:*"
                  - "iam:ListRoles"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "ssm:GetParameter"
                Resource: !Sub "arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/*"

  ClusterUserAccessReadOnlyGroup:
    Type: AWS::IAM::Group
    Properties:
      GroupName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-access-ro-group
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-access-ro-group
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - "sts:AssumeRole"
                Resource: !GetAtt ClusterUserAccessReadOnlyRole.Arn

  ClusterUserAccessReadOnlyRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-ro-access
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub arn:aws:iam::${AWS::AccountId}:root
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${CostTagValue}-cluster-user-ro-access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "eks:ListFargateProfiles"
                  - "eks:DescribeNodegroup"
                  - "eks:ListNodegroups"
                  - "eks:ListUpdates"
                  - "eks:AccessKubernetesApi"
                  - "eks:ListAddons"
                  - "eks:DescribeCluster"
                  - "eks:DescribeAddonVersions"
                  - "eks:ListClusters"
                  - "eks:ListIdentityProviderConfigs"
                  - "iam:ListRoles"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "ssm:GetParameter"
                Resource: !Sub "arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/*"

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${CostTagValue}-cluster-instance
      GroupDescription: !Sub Security group for instances in the ${CostTagValue}-cluster EKS cluster
      VpcId: !Ref VpcId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow instances to communicate with each other
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: "-1"
      FromPort: 0
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromLoadBalancer:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the load balancer
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt LoadBalancerSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: !Ref NodeProxyPort
      ToPort: !Ref NodeProxyPort

  ControlPlaneEgressToNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      Description: Allow the cluster control plane to communicate with worker Kubelet and pods
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneOn443Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow pods running extension API servers on port 443 to receive communication from cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ControlPlaneEgressToNodeSecurityGroupOn443:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      Description: Allow the cluster control plane to communicate with pods running extension API servers on port 443
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ClusterControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow pods to communicate with the cluster API Server
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      ToPort: 443
      FromPort: 443

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: !Sub ${AWS::Region}-${CostTagValue}
      VpcId: !Ref VpcId
      Protocol: HTTP
      Port: !Ref NodeProxyPort
      HealthCheckPath: /lab/{{ parameters.lab_short_name }}/hub/health
      HealthCheckIntervalSeconds: 120
      HealthyThresholdCount: 2
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  LoadBalancerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::Region}-${CostTagValue}-cluster-load-balancer
      GroupDescription: !Sub Security group for ${CostTagValue}-cluster load balancer
      VpcId: !Ref VpcId
      SecurityGroupIngress:
        - CidrIp: !Ref LoadBalancerCidrBlock
          IpProtocol: tcp
          FromPort: 80
          ToPort: 80
        - CidrIp: !Ref LoadBalancerCidrBlock
          IpProtocol: tcp
          FromPort: 443
          ToPort: 443

  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: !Sub ${CostTagValue}
      Subnets: !Ref Subnets
      SecurityGroups:
        - !GetAtt LoadBalancerSecurityGroup.GroupId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  HttpListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTP
      Port: 80
      DefaultActions:
        - Type: redirect
          RedirectConfig:
            StatusCode: HTTP_301
            Protocol: HTTPS
            Port: "443"

  HttpsListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTPS
      Port: 443
      Certificates:
        - CertificateArn: !Ref CertificateArn
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref TargetGroup

  BasicLifecycleRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-SnapshotPolicy-role
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: dlm.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSDataLifecycleManagerServiceRole

  BasicLifecyclePolicy:
    Type: "AWS::DLM::LifecyclePolicy"
    Properties:
      Description: !Sub "Lifecycle Policy for ${CostTagValue}-cluster"
      State: "ENABLED"
      ExecutionRoleArn: !GetAtt BasicLifecycleRole.Arn
      PolicyDetails:
        ResourceTypes:
          - "VOLUME"
        TargetTags:
          - Key: !Sub "kubernetes.io/cluster/${CostTagValue}-cluster"
            Value: "owned"
        Schedules:
          - Name: "Daily Snapshots"
            CreateRule:
              Interval: 24
              IntervalUnit: "HOURS"
              Times:
                - "10:00"
            RetainRule:
              Count: 1
            CopyTags: true

  CronsRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/crons"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  {% for node in nodes -%}
  {%- set node_name_escaped = node.name | regex_replace ("[^A-Za-z0-9]","00") -%}
  {%- set node_instance = node.instance.replace(' ', '').split(',') -%}

  {% if node.is_hub is defined -%}
  Repository{{ node_name_escaped }}:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/{{ node_name_escaped }}"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
  {%- endif %}

  NodeInstanceRole{{ node_name_escaped }}:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-{{ node_name_escaped }}-instance-role
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AutoScalingFullAccess
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/CloudWatchAgentAdminPolicy
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      {%- filter indent(width=6) -%}
      {% include "policies/" ~ node.node_policy ~ "_policy.yaml" %}
      {%- endfilter %}

  NodeInstanceProfile{{ node_name_escaped }}:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Sub ${AWS::Region}-${CostTagValue}-cluster-{{ node_name_escaped }}-instance-profile
      Roles:
        - !Ref NodeInstanceRole{{ node_name_escaped }}

  AutoScalingGroup{{ node_name_escaped }}:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      MixedInstancesPolicy:
        InstancesDistribution:
          OnDemandAllocationStrategy: lowest-price
        LaunchTemplate:
          LaunchTemplateSpecification:
            LaunchTemplateId: !Ref LaunchTemplate{{ node_name_escaped }}
            Version: !GetAtt  LaunchTemplate{{ node_name_escaped }}.LatestVersionNumber
          Overrides:
            {%- for instance in node_instance %}
            - InstanceType: {{ instance }}
            {%- endfor %}
      MinSize: "{{ node.min_number}}"
      MaxSize: "{{ node.max_number}}"
      VPCZoneIdentifier: !Ref ActiveSubnets
      {% if node.is_hub is defined -%}
      TargetGroupARNs:
        - !Ref TargetGroup
      {% endif -%}
      Tags:
        - Key: Name
          Value: !Sub ${CostTagValue}-{{ node_name_escaped }}-cluster-instance
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster/${CostTagValue}-cluster
          Value: owned
          PropagateAtLaunch: true
        - Key: !Sub kubernetes.io/cluster/${CostTagValue}-cluster
          Value: owned
          PropagateAtLaunch: true
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster-autoscaler/${CostTagValue}-cluster
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
          PropagateAtLaunch: true

  
  LaunchTemplate{{ node_name_escaped }}:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: !Sub ${CostTagValue}-{{ node_name_escaped }}-launch-template
      LaunchTemplateData:
        {% if node.root_volume_size is defined -%}
        BlockDeviceMappings:
          - DeviceName: /dev/xvda
            Ebs:
              VolumeType: gp2
              VolumeSize: {{ node.root_volume_size }}
              DeleteOnTermination: 'true'
              Encrypted: 'false'
        {% endif -%}
        IamInstanceProfile:
          Name: !Ref NodeInstanceProfile{{ node_name_escaped }}
        ImageId: !Ref NodeImageId
        InstanceType: {{ node_instance[0] }}
        SecurityGroupIds:
          - !GetAtt NodeSecurityGroup.GroupId
        UserData:
          Fn::Base64: !Sub |
            #!/bin/bash
            set -o xtrace
            {% if node.is_hub is defined -%}
            /etc/eks/bootstrap.sh ${CostTagValue}-cluster --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=core,server_type=core'
            {% else -%}
            /etc/eks/bootstrap.sh ${CostTagValue}-cluster --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,server_type={{ node_name_escaped }}'
            {% endif -%}
            AWS_INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
            ROOT_VOLUME_IDS=$(aws ec2 describe-instances --region ${AWS::Region} --instance-id $AWS_INSTANCE_ID --output text --query Reservations[0].Instances[0].BlockDeviceMappings[0].Ebs.VolumeId)
            aws ec2 create-tags --resources $ROOT_VOLUME_IDS --region ${AWS::Region} --tags Key=${CostTagKey},Value=${CostTagValue} Key=Name,Value=${CostTagValue}-{{ node_name_escaped }}-root
            /opt/aws/bin/cfn-signal --exit-code $? --stack ${AWS::StackName} --resource NodeGroup --region ${AWS::Region}
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: !Sub ${CostTagKey}
                Value: !Sub ${CostTagValue}
  {%- endfor %}

  ######
  # Firewall using squid. Squid is ran in an EC2 within a ASG. Configs are initially created and put into S3 via a lambda.
  #
  # Since EKS only uses one subnet (the other one required by the load balancer is ignored), we will use this subnet for the squid and rely on security groups to handle ports, etc.
  ######

  SquidSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${CostTagValue}-squid
      GroupDescription: Security group for squid
      VpcId: !Ref VpcId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  #NodeSecurityGroupFromSquid:
  #  Type: AWS::EC2::SecurityGroupIngress
  #  Properties:
  #    Description: Allow worker Kubelets and pods to receive communication from the squid
  #    GroupId: !Ref NodeSecurityGroup
  #    SourceSecurityGroupId: !GetAtt SquidSecurityGroup.GroupId
  #    IpProtocol: tcp
  #    FromPort: 3080
  #    ToPort: !Ref NodeProxyPort

  SquidConfS3Bucket:
    Type: AWS::S3::Bucket

  SquidInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/CloudWatchAgentAdminPolicy
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListObject
                Resource: !Sub '${SquidConfS3Bucket.Arn}*'
              - Effect: Allow
                Action:
                  - ec2:ModifyInstanceAttribute
                Resource: '*'

  SquidInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref SquidInstanceRole
      Path: /
  
  ### ???
#TargetGroupSquid:
#  Type: AWS::ElasticLoadBalancingV2::TargetGroup
#  Properties:
#    Name: !Sub ${CostTagValue}-tgs
#    VpcId: !Ref VpcId
#    Protocol: HTTP
#    Port: 3080
#    #HealthCheckPath: /lab/{{ parameters.lab_short_name }}/hub/health
#    #HealthCheckIntervalSeconds: 120
#    #HealthyThresholdCount: 2
#    Tags:
#      - Key: !Sub ${CostTagKey}
#        Value: !Sub ${CostTagValue}

  SquidInstanceASG:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: 1
      HealthCheckGracePeriod: 300
      HealthCheckType: EC2
      LaunchTemplate:
        LaunchTemplateId: !Ref SquidInstanceLaunchTemplate
        Version: !GetAtt SquidInstanceLaunchTemplate.LatestVersionNumber
      MaxSize: 1
      MinSize: 1
      VPCZoneIdentifier: !Ref ActiveSubnets
#      TargetGroupARNs: 
#        - !Ref TargetGroupSquid
      Tags:
        - Key: Name
          Value: !Sub 'Squid Instance - ${AWS::StackName}'
          PropagateAtLaunch: True
    #CreationPolicy:
    #  ResourceSignal:
    #    Count: 1
    #    Timeout: PT10M

  SquidInstanceLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    DependsOn:
      - SquidConfS3Bucket
    Properties:
      LaunchTemplateName: !Sub ${CostTagValue}-squid-launch-template
      LaunchTemplateData:
        IamInstanceProfile: 
          Arn: !GetAtt SquidInstanceProfile.Arn
        InstanceType: !Ref SquidInstanceType
        ImageId: !Ref SquidAmiId
        NetworkInterfaces:
          - DeviceIndex: 0
            Groups: 
              - !Ref SquidSecurityGroup
            AssociatePublicIpAddress: True
        UserData:
          Fn::Base64:
            !Sub |
              #!/bin/bash -xe
              # Redirect the user-data output to the console logs
              exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

              # Apply the latest security patches
              yum update -y --security

              # Disable source / destination check. It cannot be disabled from the launch configuration
              METADATA_TOKEN=`curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
              INSTANCE_ID=`curl -s -H "X-aws-ec2-metadata-token: $METADATA_TOKEN" http://169.254.169.254/latest/meta-data/instance-id/`
              aws ec2 modify-instance-attribute --no-source-dest-check --instance-id $INSTANCE_ID --region ${AWS::Region}

              # Install and start Squid
              yum install -y squid
              systemctl start squid || service squid start
              
              # Since the security groups handle the ports, we can ignore this setting
              #iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3129
        
              # We are not using 443 since that is terminated at the load balancer.
              #iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3130
            
              # Create a SSL certificate for the SslBump Squid module
              #mkdir /etc/squid/ssl
              #cd /etc/squid/ssl
              #openssl genrsa -out squid.key 4096
              #openssl req -new -key squid.key -out squid.csr -subj "/C=XX/ST=XX/L=squid/O=squid/CN=squid"
              #openssl x509 -req -days 3650 -in squid.csr -signkey squid.key -out squid.crt
              #cat squid.key squid.crt >> squid.pem

              # Push squid conf to S3
              echo "Pushing Squid conf to s3://${SquidConfS3Bucket}."
              
              cat > /tmp/squid.conf << 'EOF'
              visible_hostname squid
              cache deny all

              # Log format and rotation
              logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %ssl::>sni %Sh/%<a %mt
              logfile_rotate 10
              debug_options rotate=10

              # Handle HTTP requests
              http_port 3128
              http_port 3080 intercept

              # Handle HTTPS requests
              https_port 3443 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
              acl SSL_port port 443
              http_access allow SSL_port
              acl step1 at_step SslBump1
              acl step2 at_step SslBump2
              acl step3 at_step SslBump3
              ssl_bump peek step1 all

              # Deny requests to proxy instance metadata
              acl instance_metadata dst 169.254.169.254
              http_access deny instance_metadata

              # Filter HTTP requests based on the whitelist
              acl allowed_http_sites dstdomain "/etc/squid/whitelist.txt"
              http_access allow allowed_http_sites

              # Filter HTTPS requests based on the whitelist
              acl allowed_https_sites ssl::server_name "/etc/squid/whitelist.txt"
              ssl_bump peek step2 allowed_https_sites
              ssl_bump splice step3 allowed_https_sites
              ssl_bump terminate step2 all
              
              http_access deny all

              EOF
              
              aws s3 cp /tmp/squid.conf s3://${SquidConfS3Bucket}/squid.conf
              
              # Push squid conf to S3 if conf doesn't exist
              # List of whitelisted domains separated by a comma. Enter ".example.com" to whitelist all the sub-domains of example.com.
              aws s3api head-object --bucket ${SquidConfS3Bucket} --key whitelist.txt || NOT_EXIST=true
              if [ $NOT_EXIST ]; then
                echo "Whitelist text does not exist at s3://${SquidConfS3Bucket}. Pushing original copy..."
                
                cat > /tmp/whitelist.txt << 'EOF'
                .amazonaws.com
                EOF
                
                aws s3 cp /tmp/whitelist.txt s3://${SquidConfS3Bucket}/whitelist.txt
              fi

              # Refresh the Squid configuration files from S3
              mkdir /etc/squid/old
              cat > /etc/squid/squid-conf-refresh.sh << 'EOF'
              cp /etc/squid/* /etc/squid/old/
              aws s3 sync s3://${SquidConfS3Bucket} /etc/squid
              /usr/sbin/squid -k parse && /usr/sbin/squid -k reconfigure || (cp /etc/squid/old/* /etc/squid/; exit 1)
              EOF
              chmod +x /etc/squid/squid-conf-refresh.sh
              /etc/squid/squid-conf-refresh.sh

              # Schedule tasks
              cat > ~/mycron << 'EOF'
              * * * * * /etc/squid/squid-conf-refresh.sh
              0 0 * * * sleep $(($RANDOM % 3600)); yum -y update --security
              0 0 * * * /usr/sbin/squid -k rotate
              EOF
              crontab ~/mycron
              rm ~/mycron

              # Install and configure the CloudWatch Agent
              rpm -Uvh https://amazoncloudwatch-agent-${AWS::Region}.s3.${AWS::Region}.amazonaws.com/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm
              cat > /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json << 'EOF'
              {
                "agent": {
                  "metrics_collection_interval": 10,
                  "omit_hostname": true
                },
                "metrics": {
                  "metrics_collected": {
                    "procstat": [
                      {
                        "pid_file": "/var/run/squid.pid",
                        "measurement": [
                          "cpu_usage"
                        ]
                      }
                    ]
                  },
                  "append_dimensions": {
                    "AutoScalingGroupName": "${!aws:AutoScalingGroupName}"
                  },
                  "force_flush_interval": 5
                },
                "logs": {
                  "logs_collected": {
                    "files": {
                      "collect_list": [
                        {
                          "file_path": "/var/log/squid/access.log*",
                          "log_group_name": "/filtering-nat-instance/access.log",
                          "log_stream_name": "{instance_id}",
                          "timezone": "Local"
                        },
                        {
                          "file_path": "/var/log/squid/cache.log*",
                          "log_group_name": "/filtering-nat-instance/cache.log",
                          "log_stream_name": "{instance_id}",
                          "timezone": "Local"
                        }
                      ]
                    }

                  }
                }
              }
              EOF
              /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json -s

              # CloudFormation signal
              yum update -y aws-cfn-bootstrap
              /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource SquidInstanceASG --region ${AWS::Region}

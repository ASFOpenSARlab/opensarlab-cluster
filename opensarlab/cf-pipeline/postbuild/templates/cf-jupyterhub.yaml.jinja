---

{% set parameters = opensarlab['parameters'] -%}

AWSTemplateFormatVersion: 2010-09-09

Parameters:
  AdminUserName:
    Description: User name of main admin. This name is also whitelisted. Other users will need to be added via the Jupyter Hub admin console. This name MUST be a valid Earthdata user.
    Type: String
    Default: {{ parameters.admin_user_name }}

  ContainerNamespace:
    Description: The full url of the container ECR. Example-- osl-e-dev-container
    Type: String
    Default: {{ parameters.container_namespace }}

  DeploymentURL:
    Description: Jupyterhub URL used by AWS Cognito authentication. If not given, the default is the load balancer URL
    Type: String
    Default: {{ parameters.deployment_url }}

  CostTagKey:
    Type: String
    Description: "The name of the cost allocation tag you set up in AWS to track deployment costs, i.e. deployment_name"
    Default: {{ parameters.cost_tag_key }}

  CostTagValue:
    Type: String
    Description: "The value of the cost tag used for filtering the budget, etc."
    Default: {{ parameters.cost_tag_value }}

  ICALUrl:
    Type: String
    Description: "The URL of the Google Calendar used for notifications."
    Default: {{ parameters.ical_url }}

  AZPostfix:
    Type: String
    Description: "Letter postfix of AZ."
    Default: {{ parameters.az_suffix }}

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

Resources:

  HubGroupLogs:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${CostTagValue}-cluster-hub-group"
      RetentionInDays: 30

  HubProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub ${CostTagValue}-cluster-jupyterhub
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        Image: aws/codebuild/standard:3.0
        PrivilegedMode: true
      Artifacts:
        Type: CODEPIPELINE
      ServiceRole: !Sub arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-run-role
      TimeoutInMinutes: 45
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref HubGroupLogs
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub |-
              version: 0.2
              env:
                shell: bash
                secrets-manager:
                  DH_CREDS: "dockerhub/creds"
              phases:
                install:
                  runtime-versions:
                    docker: 18
                    python: 3.8
                  commands:
                    - pip3 install awscli>=1.16.158 --upgrade
                    - pip3 install boto3 --upgrade
                    - pip3 install kubernetes --upgrade
                    - pip3 install jinja2 --upgrade
                    - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/darwin/amd64/kubectl
                    - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash -s -- --version v3.7.0
                    - export HELM_HOST=127.0.0.1:44134
                    - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                    - mv /tmp/eksctl /usr/local/bin
                    - eksctl version
                pre_build:
                  commands:
                    - echo "IAM build role used is $(aws sts get-caller-identity)"
                    - echo "Logging into AWS ECR..."
                    - $(aws ecr get-login --no-include-email --region ${AWS::Region})
                    - echo "Logging into Docker Hub user to get latest jupyter image..."
                    - dh_username=$(echo $DH_CREDS | cut -f1 -d' ')
                    - echo $DH_CREDS | cut -f2 -d' ' > dh.pass
                    - cat dh.pass | docker login -u $dh_username --password-stdin
                    - echo "Install helm charts and update..."
                    - helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
                    - helm repo add autoscaler https://kubernetes.github.io/autoscaler
                    - helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
                    - helm repo update
                build:
                  commands:
                    - cd $CODEBUILD_SRC_DIR/opensarlab/cf-cluster/;
                    - echo "Update assumed role of cluster-build...";
                      cp configs/cluster-build-assumerolepolicy.template.json assume.json;
                      sed -i "s|CLUSTER_RUN_ARN|arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-run-role|" assume.json;
                      aws iam update-assume-role-policy --role-name ${AWS::Region}-${CostTagValue}-cluster-build-role --policy-document file://assume.json;
                      sleep 5
                    - echo "Update kubeconfig and apply config files...";
                      aws eks update-kubeconfig --name ${CostTagValue}-cluster --role-arn arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-build-role;
                    - echo "Apply aws-auth.yaml...";
                      kubectl apply -f configs/aws-auth-cm.yaml
                    - echo "Reapply storage class...";
                      kubectl delete sc gp2 || true;
                      kubectl apply -f configs/csi-sc.yaml
                    - echo "Apply ebs csi driver...";
                      helm upgrade aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver
                        --install
                        --version 2.4.0
                        --namespace kube-system
                        --timeout=6m0s
                        --atomic
                        --set enableVolumeScheduling=true
                        --set enableVolumeResizing=true
                        --set enableVolumeSnapshot=false
                        --set extraCreateMetadata=true
                        --set k8sTagClusterId=${CostTagValue}-cluster
                        --set extraVolumeTags.${CostTagKey}=${CostTagValue}
                    - echo "Associate IAM and OIDC for service accounts...";
                      eksctl utils associate-iam-oidc-provider --cluster ${CostTagValue}-cluster --approve
                    - echo "Get registry URI...";
                      export REGISTRY_URI='${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ContainerNamespace}'
                      echo $REGISTRY_URI
                    - echo "Build Hub Image...";
                      PrevWD=$(pwd);
                      cd hub;
                      cp dockerfile dockerfile.build;
                      export HUB_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/hub:$HUB_IMAGE_BUILD -t $REGISTRY_URI/hub:latest .;
                      docker push $REGISTRY_URI/hub:$HUB_IMAGE_BUILD;
                      docker push $REGISTRY_URI/hub:latest;
                      cd $PrevWD;
                    - echo "Base64 convert images..."
                      base64 ./hub/helm_config.d/usr/local/share/jupyterhub/static/images/ASFLogo-Blue2.png > ASFLogo-Blue2.png.b64;
                      base64 ./hub/helm_config.d/usr/local/share/jupyterhub/static/images/alaska_satellite_facility_1000.png > alaska_satellite_facility_1000.png.b64;
                    - echo "Install JupyterHub cluster...";
                      helm upgrade jupyter jupyterhub/jupyterhub
                        --install
                        --create-namespace
                        --namespace jupyter
                        --version 1.1.3
                        --values ./hub/helm_config.yaml
                        --timeout=6m0s
                        --atomic
                        --set hub.image.name=$REGISTRY_URI/hub,hub.image.tag=$HUB_IMAGE_BUILD
                        --set hub.config.Authenticator.admin_users[0]='${AdminUserName}'
                        --set proxy.service.nodePorts.http='${NodeProxyPort}'
                        --set custom.REGISTRY_URI=$REGISTRY_URI
                        --set custom.OAUTH_JUPYTER_URL='${DeploymentURL}'
                        --set custom.OAUTH_DNS_NAME='https://${CostTagValue}-cluster.auth.${AWS::Region}.amazoncognito.com'
                        --set custom.CLUSTER_NAME='${CostTagValue}-cluster',custom.OAUTH_POOL_NAME='${CostTagValue}-cluster',custom.AZ_NAME='${AWS::Region}${AZPostfix}'
                        --set custom.COST_TAG_VALUE='${CostTagValue}'
                        --set custom.COST_TAG_KEY='${CostTagKey}'
                        --set-file hub.extraFiles.hooks.stringData='./hub/helm_config.d/hooks.py'
                        --set-file hub.extraFiles.hooks-pv.stringData='./hub/helm_config.d/hooks/pv.yaml'
                        --set-file hub.extraFiles.hooks-pvc.stringData='./hub/helm_config.d/hooks/pvc.yaml'
                        --set-file hub.extraFiles.profiles.stringData='./hub/helm_config.d/profiles.py'
                        --set-file hub.extraFiles.services.stringData='./hub/helm_config.d/services.py'
                        --set-file hub.extraFiles.lib-logout.stringData='./hub/helm_config.d/usr/local/lib/generic_with_logout.py'
                        --set-file hub.extraFiles.lib-groups.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/groups.py'
                        --set-file hub.extraFiles.lib-handlers-groups.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/handlers/groups.py'
                        --set-file hub.extraFiles.lib-handlers-init.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/handlers/__init__.py'
                        --set-file hub.extraFiles.share-images-alaska_satellite_facility_1000.binaryData='./alaska_satellite_facility_1000.png.b64'
                        --set-file hub.extraFiles.share-images-blue2.binaryData='./ASFLogo-Blue2.png.b64'
                        --set-file hub.extraFiles.share-nasa_logo.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/static/images/NASA_logo.svg'
                        --set-file hub.extraFiles.share-js-groups.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/static/js/groups.js'
                        --set-file hub.extraFiles.share-templates-groups.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/groups.html'
                        --set-file hub.extraFiles.share-templates-login.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/login.html'
                        --set-file hub.extraFiles.share-templates-pending.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/pending.html'
                        --set-file hub.extraFiles.share-templates-custom-page.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/custom/page.html'
                        --set-file singleuser.extraFiles.user-hooks-sar.stringData='./hub/helm_config.d/user_containers/hooks/sar.sh'
                        --set-file singleuser.extraFiles.user-hooks-sar_test.stringData='./hub/helm_config.d/user_containers/hooks/sar_test.sh'
                        --set-file singleuser.extraFiles.user-hooks-sar_dev.stringData='./hub/helm_config.d/user_containers/hooks/sar_dev.sh'
                        --set-file singleuser.extraFiles.user-hooks-no_smart_git.stringData='./hub/helm_config.d/user_containers/hooks/no_smart_git.sh'
                        --set-file singleuser.extraFiles.user-hooks-pull.stringData='./hub/helm_config.d/user_containers/hooks/pull.py'
                        --set-file singleuser.extraFiles.user-others-check_storage.stringData='./hub/helm_config.d/user_containers/others/check_storage.py'
                        --set-file singleuser.extraFiles.user-custom_magics.stringData='./hub/helm_config.d/user_containers/custom_magics/00-df.py'
                    - kubectl -n jupyter rollout status -w deployment.apps/hub
                    - echo "Associate volume provisioner with service account...";
                      kubectl create clusterrolebinding cluster-pv --clusterrole=system:persistent-volume-provisioner --serviceaccount=jupyter:hub --dry-run -o yaml | kubectl apply -f -
                    - echo "Apply CNI. This must be updated on every cluster upgrade...";
                      curl -o aws-k8s-cni.yaml https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.9/config/v1.9/aws-k8s-cni.yaml;
                      sed -i -e 's/us-west-2/${AWS::Region}/' aws-k8s-cni.yaml;
                      kubectl apply -f aws-k8s-cni.yaml
                    - echo "Install autoscaler...";
                      helm upgrade autoscaler autoscaler/cluster-autoscaler-chart
                        --install
                        --create-namespace
                        --namespace autoscaler
                        --atomic
                        --timeout=2m0s
                        --set autoDiscovery.clusterName=${CostTagValue}-cluster,awsRegion=${AWS::Region},nodeSelector."hub\\.jupyter\\.org/node-purpose"=core
                    - echo "Install Fluent Bit for AWS Container Insights...";
                      export FluentBitHttpPort='2020';
                      export FluentBitReadFromHead='Off';
                      [[ $FluentBitReadFromHead = 'On' ]] && FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On';
                      [[ -z $FluentBitHttpPort ]] && FluentBitHttpServer='Off' || FluentBitHttpServer='On';
                      curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml
                      | sed 's/{% raw %}{{cluster_name}}{% endraw %}/${CostTagValue}-cluster/;
                        s/{% raw %}{{region_name}}{% endraw %}/${AWS::Region}/;
                        s/{% raw %}{{http_server_toggle}}{% endraw %}/"'$FluentBitHttpServer'"/;
                        s/{% raw %}{{http_server_port}}{% endraw %}/"'$FluentBitHttpPort'"/;
                        s/{% raw %}{{read_from_head}}{% endraw %}/"'$FluentBitReadFromHead'"/;
                        s/{% raw %}{{read_from_tail}}{% endraw %}/"'$FluentBitReadFromTail'"/'
                      | kubectl apply -f -
                    - echo "Create services namespace";
                      kubectl apply -f services/namespace.yaml
                    - echo "Build Notification Image...";
                      PrevWD=$(pwd); 
                      cd services/notifications;
                      cp dockerfile dockerfile.build;
                      export NOTES_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD -t $REGISTRY_URI/notifications:latest .;
                      docker push $REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD;
                      docker push $REGISTRY_URI/notifications:latest;
                      cd $PrevWD;
                    - echo "Install notifications service...";
                      cp services/notifications/k8s/deployment.example.yaml services/notifications/k8s/deployment.yaml;
                      sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD|" services/notifications/k8s/deployment.yaml;
                      sed -i "s|ICAL_URL_PLACEHOLDER|${ICALUrl}|" services/notifications/k8s/deployment.yaml;
                      kubectl apply -f services/notifications/k8s/service.yaml;
                      kubectl apply -f services/notifications/k8s/deployment.yaml;
                      kubectl -n services rollout status deployment/notifications -w;
                      kubectl -n services scale deployment notifications --replicas=0;
                      kubectl -n services scale deployment notifications --replicas=1
                    - echo "Build Crons Image...";
                      PrevWD=$(pwd); 
                      cd services/crons;
                      cp dockerfile dockerfile.build;
                      export CRONS_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD -t $REGISTRY_URI/crons:latest .;
                      docker push $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD;
                      docker push $REGISTRY_URI/crons:latest;
                      cd $PrevWD;
                    - echo "Install Crons service...";
                      cp services/crons/k8s/deployment.example.yaml services/crons/k8s/deployment.yaml;
                      sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/crons:$CRONS_IMAGE_BUILD|" services/crons/k8s/deployment.yaml;
                      sed -i "s|REGION_NAME_PLACEHOLDER|${AWS::Region}|" services/crons/k8s/deployment.yaml;
                      sed -i "s|COGNITO_NAME_PLACEHOLDER|${CostTagValue}-cluster|" services/crons/k8s/deployment.yaml;
                      sed -i "s|CLUSTER_NAME_PLACEHOLDER|${CostTagValue}-cluster|" services/crons/k8s/deployment.yaml;
                      sed -i "s|NAMESPACE_PLACEHOLDER|jupyter|" services/crons/k8s/deployment.yaml;
                      sed -i "s|DAYS_VOL_INACTIVE_TILL_TERMINATION_PLACEHOLDER|5|" services/crons/k8s/deployment.yaml;
                      kubectl apply -f services/crons/k8s/deployment.yaml;
                      kubectl -n services rollout status deployment/crons -w;
                      kubectl -n services scale deployment crons --replicas=0;
                      kubectl -n services scale deployment crons --replicas=1

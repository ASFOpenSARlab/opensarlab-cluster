AWSTemplateFormatVersion: 2010-09-09

Parameters:
  AdminUserName:
    Description: User name of main admin. This name is also whitelisted. Other users will need to be added via the Jupyter Hub admin console. This name MUST be a valid Earthdata user.
    Type: String

  CertificateArn:
    Description: The ARN of the SSL certificate attached to the load balancer.
    Type: String
    Default: ""

  ContainerNamespace:
    Description: OVERRIDE in child. The full url of the container ECR. Example-- osl-e-dev-container
    Type: String
    Default: ""

  OAuthPoolName:
    Description: AWS Cognito User Pool name
    Type: String
    Default: ""

  OAuthDNSName:
    Description: AWS Cognito User Pool DNS Url
    Type: String
    Default: ""

  DeploymentURL:
    Description: Jupyterhub URL used by AWS Cognito authentication. If not given, the default is the load balancer URL
    Type: String
    Default: ""

  CostTagKey:
    Type: String
    Description: "The name of the cost allocation tag you set up in AWS to track deployment costs, i.e. deployment_name"
    Default: ""

  CostTagValue:
    Type: String
    Description: "The value of the cost tag used for filtering the budget, etc."
    Default: "osl-daac"

  ICALUrl:
    Type: String
    Description: "The URL of the Google Calendar used for notifications."
    Default: ""

  # The following are dynamically added by the pipeline
  VpcId:
    Description: The VPC of the worker instances
    Type: AWS::EC2::VPC::Id

  Subnets:
    Description: The subnets where workers can be created.
    Type: List<AWS::EC2::Subnet::Id>

  ActiveSubnets:
    Description: The subnets actually used by resources in the cluster. Typically only one (e.g., subnet for AZ -d).
    Type: List<AWS::EC2::Subnet::Id>

  # The following values require defaults.
  LoadBalancerCidrBlock:
    Description: The range of allowed IPv4 addresses for the load balancer. This only firewalls the load balancer URL and not the cluster in general.
    Type: String
    Default: 0.0.0.0/0

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

  NodeImageId:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.21/amazon-linux-2/recommended/image_id

  NodeInstanceTypeCore:
    Description: EC2 instance type for the node Core instances.
    Type: String
    Default: t3a.medium

  NodeInstanceTypeSAR1:
    Description: EC2 instance type for SAR 1.
    Type: String
    Default: m5a.2xlarge

  NodeInstanceTypeSAR2:
    Description: EC2 instance type for SAR 2.
    Type: String
    Default: m5a.8xlarge

Outputs:
  AppUrl:
    Value: !GetAtt LoadBalancer.DNSName

Resources:
  ClusterRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${AWS::StackName}
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - eks.amazonaws.com
                - codebuild.amazonaws.com
                - cloudformation.amazonaws.com
                - codepipeline.amazonaws.com
                - iam.amazonaws.com
            Action: sts:AssumeRole
          - Effect: Allow
            Principal:
              AWS:
                Fn::ImportValue: !Sub ${AWS::Region}-${AWS::StackName}-${AWS::AccountId}-common-role-arn
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
        - arn:aws:iam::aws:policy/AmazonEKSServicePolicy
        - arn:aws:iam::aws:policy/AutoScalingFullAccess
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${AWS::StackName}-hub-build
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "codestar-connections:*"
                  - "s3:*"
                  - "dlm:*"
                  - "logs:*"
                  - "cloudformation:*"
                  - "elasticloadbalancing:*"
                  - "autoscaling:*"
                  - "codebuild:*"
                  - "iam:*"
                  - "secretsmanager:*"
                  - "ssm:*"
                  - "ecr:*"
                  - "ec2:*"
                  - "eks:*"
                  - "sts:AssumeRole"
                Resource: "*"

  ClusterSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}
      GroupDescription: !Sub Security group for the ${AWS::StackName} EKS cluster
      VpcId: !Ref VpcId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  Cluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name: !Ref AWS::StackName
      RoleArn: !GetAtt ClusterRole.Arn
      ResourcesVpcConfig:
        SubnetIds: !Ref Subnets
        SecurityGroupIds:
          - !GetAtt ClusterSecurityGroup.GroupId
      Version: "1.21"

  UserAccessRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${Cluster}-access
      AssumeRolePolicyDocument:
        Statement:
          Effect: Deny
          Principal:
            AWS: !Sub arn:aws:iam::${AWS::AccountId}:root
          Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${Cluster}-access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "eks:*"
                Resource: "*"

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${Cluster}-instance
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AutoScalingFullAccess
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
      Policies:
        - PolicyName: !Sub ${AWS::Region}-${Cluster}-Service_Account_Policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Sid: SAHubDeleteVolumes
                Effect: Allow
                Action:
                  - ec2:DescribeSnapshots
                  - ec2:DescribeVolumes
                Resource: "*"
              - Sid: SAHubDeleteSnapshots
                Effect: Allow
                Action:
                  - ec2:DescribeSnapshots
                  - ec2:DeleteSnapshot
                  - ec2:DescribeVolumes
                  - cognito-idp:ListUserPools
                  - cognito-idp:AdminGetUser
                  - cognito-idp:AdminDisableUser
                  - cognito-idp:ListUsers
                  - ses:SendEmail
                Resource: "*"
              - Sid: SAHubForceDeactivation
                Effect: Allow
                Action:
                  - ec2:DescribeSnapshots
                  - ec2:DeleteSnapshot
                  - cognito-idp:ListUserPools
                  - cognito-idp:ListUsers
                  - cognito-idp:AdminDisableUser
                Resource: "*"
              - Sid: SAHubVolumeFromSnapshot
                Effect: Allow
                Action:
                  - ec2:DescribeSnapshots
                  - ec2:CreateVolume
                  - ec2:CreateTags
                Resource: "*"
              - Sid: SAHubVolumeStoppingTags
                Effect: Allow
                Action:
                  - ec2:DescribeVolumes
                  - ec2:CreateTags
                Resource: "*"
              - Sid: SAHubGenericWithLogout
                Effect: Allow
                Action:
                  - cognito-idp:ListUserPools
                  - cognito-idp:ListUserPoolClients
                  - cognito-idp:DescribeUserPoolClient
                Resource: "*"
        - PolicyName: !Sub ${AWS::Region}-${Cluster}-EKS_CSI_EBS_Policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - ec2:AttachVolume
                  - ec2:CreateSnapshot
                  - ec2:CreateTags
                  - ec2:CreateVolume
                  - ec2:DeleteSnapshot
                  - ec2:DeleteTags
                  - ec2:DeleteVolume
                  - ec2:DescribeAvailabilityZones
                  - ec2:DescribeInstances
                  - ec2:DescribeSnapshots
                  - ec2:DescribeTags
                  - ec2:DescribeVolumes
                  - ec2:DescribeVolumesModifications
                  - ec2:DetachVolume
                  - ec2:ModifyVolume
                Resource: "*"

  NodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref NodeInstanceRole
      Roles:
        - !Ref NodeInstanceRole

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${Cluster}-instance
      GroupDescription: !Sub Security group for instances in the ${Cluster} EKS cluster
      VpcId: !Ref VpcId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow instances to communicate with each other
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: "-1"
      FromPort: 0
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromLoadBalancer:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the load balancer
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt LoadBalancerSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: !Ref NodeProxyPort
      ToPort: !Ref NodeProxyPort

  ControlPlaneEgressToNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      Description: Allow the cluster control plane to communicate with worker Kubelet and pods
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneOn443Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow pods running extension API servers on port 443 to receive communication from cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ControlPlaneEgressToNodeSecurityGroupOn443:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      Description: Allow the cluster control plane to communicate with pods running extension API servers on port 443
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ClusterControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow pods to communicate with the cluster API Server
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      ToPort: 443
      FromPort: 443

  AutoScalingGroupCore:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: "1"
      LaunchConfigurationName: !Ref LaunchConfigurationCore
      MinSize: "1"
      MaxSize: "2"
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
        - !Ref TargetGroup
      Tags:
        - Key: Name
          Value: !Sub ${Cluster}-instance
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster/${Cluster}
          Value: owned
          PropagateAtLaunch: true
        - Key: !Sub kubernetes.io/cluster/${Cluster}
          Value: owned
          PropagateAtLaunch: true
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
          PropagateAtLaunch: true

  LaunchConfigurationCore:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceTypeCore
      SecurityGroups:
        - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=core,server_type=core'
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${Cluster} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupSAR1:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: "0"
      LaunchConfigurationName: !Ref LaunchConfigurationSAR1
      MinSize: "0"
      MaxSize: "25"
      VPCZoneIdentifier: !Ref ActiveSubnets
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-instance
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster/${Cluster}
          Value: owned
          PropagateAtLaunch: true
        - Key: !Sub kubernetes.io/cluster/${Cluster}
          Value: owned
          PropagateAtLaunch: true
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
          PropagateAtLaunch: true

  LaunchConfigurationSAR1:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceTypeSAR1
      SecurityGroups:
        - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,server_type=sar_1'
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupSAR2:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: "0"
      LaunchConfigurationName: !Ref LaunchConfigurationSAR2
      MinSize: "0"
      MaxSize: "25"
      VPCZoneIdentifier: !Ref ActiveSubnets
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-instance
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster/${Cluster}
          Value: owned
          PropagateAtLaunch: true
        - Key: !Sub kubernetes.io/cluster/${Cluster}
          Value: owned
          PropagateAtLaunch: true
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
          PropagateAtLaunch: true

  LaunchConfigurationSAR2:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceTypeSAR2
      SecurityGroups:
        - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,server_type=sar_2'
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: !Ref AWS::StackName
      VpcId: !Ref VpcId
      Protocol: HTTP
      Port: !Ref NodeProxyPort
      HealthCheckPath: /hub/health
      HealthCheckIntervalSeconds: 120
      HealthyThresholdCount: 2
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  LoadBalancerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${Cluster}-load-balancer
      GroupDescription: !Sub Security group for ${Cluster} load balancer
      VpcId: !Ref VpcId
      SecurityGroupIngress:
        - CidrIp: !Ref LoadBalancerCidrBlock
          IpProtocol: tcp
          FromPort: 80
          ToPort: 80
        - CidrIp: !Ref LoadBalancerCidrBlock
          IpProtocol: tcp
          FromPort: 443
          ToPort: 443

  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: !Ref AWS::StackName
      Subnets: !Ref Subnets
      SecurityGroups:
        - !GetAtt LoadBalancerSecurityGroup.GroupId
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  HttpListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTP
      Port: 80
      DefaultActions:
        - Type: redirect
          RedirectConfig:
            StatusCode: HTTP_301
            Protocol: HTTPS
            Port: "443"

  HttpsListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTPS
      Port: 443
      Certificates:
        - CertificateArn: !Ref CertificateArn
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref TargetGroup

  BasicLifecycleRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${AWS::StackName}-SnapshotPolicyRole
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: dlm.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSDataLifecycleManagerServiceRole

  BasicLifecyclePolicy:
    Type: "AWS::DLM::LifecyclePolicy"
    Properties:
      Description: !Sub "Lifecycle Policy for ${Cluster}"
      State: "ENABLED"
      ExecutionRoleArn: !GetAtt BasicLifecycleRole.Arn
      PolicyDetails:
        ResourceTypes:
          - "VOLUME"
        TargetTags:
          - Key: !Sub "kubernetes.io/cluster/${Cluster}"
            Value: "owned"
        Schedules:
          - Name: "Daily Snapshots"
            CreateRule:
              Interval: 24
              IntervalUnit: "HOURS"
              Times:
                - "10:00"
            RetainRule:
              Count: 1
            CopyTags: true

  NotificationsRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/notifications"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  CronsRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/crons"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  HubRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/hub"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  HubGroupLogs:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${Cluster}-hub-group"
      RetentionInDays: 30

  HubProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub ${Cluster}-jupyterhub
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        Image: aws/codebuild/standard:3.0
        PrivilegedMode: True
      Artifacts:
        Type: CODEPIPELINE
      ServiceRole: !Ref ClusterRole
      TimeoutInMinutes: 45
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
      Source:
        Type: CODEPIPELINE
        BuildSpec:
          Fn::Sub:
            - |-
              version: 0.2
              env:
                shell: bash
              phases:
                install:
                  runtime-versions:
                    docker: 18
                    python: 3.8
                  commands:
                    - pip3 install awscli>=1.16.158 --upgrade
                    - pip3 install boto3 --upgrade
                    - pip3 install kubernetes --upgrade
                    - pip3 install jinja2 --upgrade
                    - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/darwin/amd64/kubectl
                    - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash -s -- --version v3.7.0
                    - export HELM_HOST=127.0.0.1:44134
                    - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                    - mv /tmp/eksctl /usr/local/bin
                    - eksctl version
                pre_build:
                  commands:
                    - echo "Logging into AWS ECR..."
                    - $(aws ecr get-login --no-include-email --region ${AWS::Region})
                    - echo "Logging into Docker Hub user to get latest jupyter image..."
                    - dh_creds=$(aws secretsmanager get-secret-value --secret-id dockerhub/creds --query 'SecretString' | sed 's/\"//g' )
                    - dh_username=$(echo $dh_creds | cut -f1 -d' ')
                    - echo $dh_creds | cut -f2 -d' ' > dh.pass
                    - cat dh.pass | docker login -u $dh_username --password-stdin
                    - echo "Install helm charts and update..."
                    - helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
                    - helm repo add autoscaler https://kubernetes.github.io/autoscaler
                    - helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
                    - helm repo update
                build:
                  commands:
                    - echo "Update assume role of Common role..."
                    - sed -i "s|CLUSTER_ROLE_ARN|${ClusterRole.Arn}|" common-role-assumerolepolicy-template.json;
                      export commonrolearn=${CommonRoleArn}; echo $commonrolearn;
                      aws iam update-assume-role-policy --role-name ${!commonrolearn##*/} --policy-document file://common-role-assumerolepolicy-template.json;
                      sleep 5
                    - echo "Update kubeconfig and config files..."
                    - aws eks update-kubeconfig --name ${Cluster} --role-arn ${CommonRoleArn}
                    - sed -i "s|INSTANCE_ROLE_ARN|${NodeInstanceRole.Arn}|" configs/aws-auth-cm.yaml
                    - sed -i "s|USER_ACCESS_ROLE_ARN|${UserAccessRole.Arn}|" configs/aws-auth-cm.yaml
                    - kubectl apply -f configs/aws-auth-cm.yaml
                    - echo "Reapply storage class..."
                    - kubectl delete sc gp2 || true
                    - echo "Install EBS CSI driver for enhanced volume management..."
                    - kubectl apply -f configs/csi-sc.yaml
                    - helm upgrade aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver
                        --install
                        --version 2.4.0
                        --namespace kube-system
                        --timeout=6m0s
                        --atomic
                        --set enableVolumeScheduling=true
                        --set enableVolumeResizing=true
                        --set enableVolumeSnapshot=false
                        --set extraCreateMetadata=true
                        --set k8sTagClusterId=${Cluster}
                        --set extraVolumeTags.${CostTagKey}=${CostTagValue}
                    - echo "Associate IAM and OIDC for service accounts..."
                    - eksctl utils associate-iam-oidc-provider --cluster ${Cluster} --approve
                    - echo "Get registry URI...";
                      export REGISTRY_URI='${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ContainerNamespace}'
                      echo $REGISTRY_URI
                    - echo "Build Hub Image...";
                      PrevWD=$(pwd); 
                      cd hub;
                      cp dockerfile dockerfile.build;
                      export HUB_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/hub:$HUB_IMAGE_BUILD -t $REGISTRY_URI/hub:latest .;
                      docker push $REGISTRY_URI/hub:$HUB_IMAGE_BUILD;
                      docker push $REGISTRY_URI/hub:latest;
                      cd $PrevWD;
                    - echo "Install JupyterHub cluster...";
                      base64 ./hub/helm_config.d/usr/local/share/jupyterhub/static/images/ASFLogo-Blue2.png > ASFLogo-Blue2.png.b64 &&
                      base64 ./hub/helm_config.d/usr/local/share/jupyterhub/static/images/alaska_satellite_facility_1000.png > alaska_satellite_facility_1000.png.b64 &&
                      helm upgrade jupyter jupyterhub/jupyterhub
                        --install
                        --create-namespace
                        --namespace jupyter
                        --version 1.1.3
                        --values ./hub/helm_config.yaml
                        --timeout=6m0s
                        --atomic
                        --set hub.image.name=$REGISTRY_URI/hub,hub.image.tag=$HUB_IMAGE_BUILD
                        --set hub.config.Authenticator.admin_users[0]='${AdminUserName}'
                        --set proxy.service.nodePorts.http='${NodeProxyPort}'
                        --set custom.REGISTRY_URI=$REGISTRY_URI
                        --set custom.OAUTH_JUPYTER_URL=$( [ -n '${DeploymentURL}' ] && echo '${DeploymentURL}' || echo 'https://${LoadBalancer.DNSName}' )
                        --set custom.OAUTH_DNS_NAME='${OAuthDNSName}'
                        --set custom.CLUSTER_NAME='${Cluster}',custom.OAUTH_POOL_NAME='${OAuthPoolName}',custom.AZ_NAME='${AWS::Region}d'
                        --set custom.COST_TAG_VALUE='${CostTagValue}'
                        --set custom.COST_TAG_KEY='${CostTagKey}'
                        --set-file hub.extraFiles.hooks.stringData='./hub/helm_config.d/hooks.py'
                        --set-file hub.extraFiles.hooks-pv.stringData='./hub/helm_config.d/hooks/pv.yaml'
                        --set-file hub.extraFiles.hooks-pvc.stringData='./hub/helm_config.d/hooks/pvc.yaml'
                        --set-file hub.extraFiles.profiles.stringData='./hub/helm_config.d/profiles.py'
                        --set-file hub.extraFiles.services.stringData='./hub/helm_config.d/services.py'
                        --set-file hub.extraFiles.lib-logout.stringData='./hub/helm_config.d/usr/local/lib/generic_with_logout.py'
                        --set-file hub.extraFiles.lib-groups.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/groups.py'
                        --set-file hub.extraFiles.lib-handlers-groups.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/handlers/groups.py'
                        --set-file hub.extraFiles.lib-handlers-init.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/handlers/__init__.py'
                        --set-file hub.extraFiles.share-images-alaska_satellite_facility_1000.binaryData='./alaska_satellite_facility_1000.png.b64'
                        --set-file hub.extraFiles.share-images-blue2.binaryData='./ASFLogo-Blue2.png.b64'
                        --set-file hub.extraFiles.share-nasa_logo.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/static/images/NASA_logo.svg'
                        --set-file hub.extraFiles.share-js-groups.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/static/js/groups.js'
                        --set-file hub.extraFiles.share-templates-groups.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/groups.html'
                        --set-file hub.extraFiles.share-templates-login.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/login.html'
                        --set-file hub.extraFiles.share-templates-pending.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/pending.html'
                        --set-file hub.extraFiles.share-templates-custom-page.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/custom/page.html'
                    - kubectl -n jupyter rollout status -w deployment.apps/hub
                    - echo "Associate volume provisioner with service account...";
                      kubectl create clusterrolebinding cluster-pv --clusterrole=system:persistent-volume-provisioner --serviceaccount=jupyter:hub --dry-run -o yaml | kubectl apply -f -
                    - echo "Apply CNI. This must be updated on every upgrade...";
                      curl -o aws-k8s-cni.yaml https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.9/config/v1.9/aws-k8s-cni.yaml;
                      sed -i -e 's/us-west-2/${AWS::Region}/' aws-k8s-cni.yaml;
                      kubectl apply -f aws-k8s-cni.yaml
                    - echo "Install autoscaler...";
                      helm upgrade autoscaler autoscaler/cluster-autoscaler-chart
                        --install
                        --create-namespace
                        --namespace autoscaler
                        --atomic
                        --timeout=2m0s
                        --set autoDiscovery.clusterName=${Cluster},awsRegion=${AWS::Region},nodeSelector."hub\\.jupyter\\.org/node-purpose"=core
                    - echo "Install Fluent Bit for AWS Container Insights...";
                      export FluentBitHttpPort='2020';
                      export FluentBitReadFromHead='Off';
                      [[ $FluentBitReadFromHead = 'On' ]] && FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On';
                      [[ -z $FluentBitHttpPort ]] && FluentBitHttpServer='Off' || FluentBitHttpServer='On';
                      curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml | sed 's/{{cluster_name}}/${Cluster}/;s/{{region_name}}/${AWS::Region}/;s/{{http_server_toggle}}/"'$FluentBitHttpServer'"/;s/{{http_server_port}}/"'$FluentBitHttpPort'"/;s/{{read_from_head}}/"'$FluentBitReadFromHead'"/;s/{{read_from_tail}}/"'$FluentBitReadFromTail'"/' | kubectl apply -f -
                    - echo "Create services namespace";
                      kubectl apply -f services/namespace.yaml
                    - echo "Build Notification Image...";
                      PrevWD=$(pwd); 
                      cd services/notifications;
                      cp dockerfile dockerfile.build;
                      export NOTES_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD -t $REGISTRY_URI/notifications:latest .;
                      docker push $REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD;
                      docker push $REGISTRY_URI/notifications:latest;
                      cd $PrevWD;
                    - echo "Install notifications service...";
                      cp services/notifications/k8s/deployment.example.yaml services/notifications/k8s/deployment.yaml;
                      sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD|" services/notifications/k8s/deployment.yaml;
                      sed -i "s|ICAL_URL_PLACEHOLDER|${ICALUrl}|" services/notifications/k8s/deployment.yaml;
                      kubectl apply -f services/notifications/k8s/service.yaml;
                      kubectl apply -f services/notifications/k8s/deployment.yaml;
                      kubectl -n services rollout status deployment/notifications -w;
                      kubectl -n services scale deployment notifications --replicas=0;
                      kubectl -n services scale deployment notifications --replicas=1
                    - echo "Build Crons Image...";
                      PrevWD=$(pwd); 
                      cd services/crons;
                      cp dockerfile dockerfile.build;
                      export CRONS_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD -t $REGISTRY_URI/crons:latest .;
                      docker push $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD;
                      docker push $REGISTRY_URI/crons:latest;
                      cd $PrevWD;
                    - echo "Install Crons service...";
                      cp services/crons/k8s/deployment.example.yaml services/crons/k8s/deployment.yaml;
                      sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/crons:$CRONS_IMAGE_BUILD|" services/crons/k8s/deployment.yaml;
                      sed -i "s|REGION_NAME_PLACEHOLDER|${AWS::Region}|" services/crons/k8s/deployment.yaml;
                      sed -i "s|COGNITO_NAME_PLACEHOLDER|${OAuthPoolName}|" services/crons/k8s/deployment.yaml;
                      sed -i "s|CLUSTER_NAME_PLACEHOLDER|${Cluster}|" services/crons/k8s/deployment.yaml;
                      sed -i "s|NAMESPACE_PLACEHOLDER|jupyter|" services/crons/k8s/deployment.yaml;
                      sed -i "s|DAYS_VOL_INACTIVE_TILL_TERMINATION_PLACEHOLDER|5|" services/crons/k8s/deployment.yaml;
                      kubectl apply -f services/crons/k8s/deployment.yaml;
                      kubectl -n services rollout status deployment/crons -w;
                      kubectl -n services scale deployment crons --replicas=0;
                      kubectl -n services scale deployment crons --replicas=1
            - CommonRoleArn:
                Fn::ImportValue: !Sub ${AWS::Region}-${AWS::StackName}-${AWS::AccountId}-common-role-arn

      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref HubGroupLogs

---

{% set parameters = opensarlab['parameters'] -%}
{% set nodes = opensarlab['nodes'] -%}
{% set profiles = opensarlab['profiles'] -%}
{% set others = opensarlab['others'] -%}

AWSTemplateFormatVersion: 2010-09-09

Parameters:
  AdminUserName:
    Description: User name of main admin. This name is also whitelisted. Other users will need to be added via the Jupyter Hub admin console. This name MUST be a valid Earthdata user.
    Type: String
    Default: {{ parameters.admin_user_name }}

  CertificateArn:
    Description: The ARN of the SSL certificate attached to the load balancer.
    Type: String
    Default: {{ parameters.certificate_arn }}

  ContainerNamespace:
    Description: The full url of the container ECR. Example-- osl-e-dev-container
    Type: String
    Default: {{ parameters.container_namespace }}

  OAuthPoolName:
    Description: AWS Cognito User Pool name
    Type: String
    Default: {{ parameters.oauth_pool_name }}

  OAuthDNSName:
    Description: AWS Cognito User Pool DNS Url
    Type: String
    Default: {{ parameters.oauth_dns_name }}

  DeploymentURL:
    Description: Jupyterhub URL used by AWS Cognito authentication. If not given, the default is the load balancer URL
    Type: String
    Default: {{ parameters.deployment_url }}

  CostTagKey:
    Type: String
    Description: "The name of the cost allocation tag you set up in AWS to track deployment costs, i.e. deployment_name"
    Default: {{ parameters.cost_tag_key }}

  CostTagValue:
    Type: String
    Description: "The value of the cost tag used for filtering the budget, etc."
    Default: {{ parameters.cost_tag_value }}

  ICALUrl:
    Type: String
    Description: "The URL of the Google Calendar used for notifications."
    Default: {{ parameters.ical_url }}

  # The following are dynamically added by the pipeline
  VpcId:
    Description: The VPC of the worker instances
    Type: AWS::EC2::VPC::Id
    Default: {{ others.vpc_id }}

  Subnets:
    Description: The subnets where workers can be created.
    Type: List<AWS::EC2::Subnet::Id>
    Default: {{ others.all_subnets }}

  ActiveSubnets:
    Description: The subnets actually used by resources in the cluster. Typically only one (e.g., subnet for AZ -d).
    Type: List<AWS::EC2::Subnet::Id>
    Default: {{ others.active_subnets }}

  # The following values require defaults.
  LoadBalancerCidrBlock:
    Description: The range of allowed IPv4 addresses for the load balancer. This only firewalls the load balancer URL and not the cluster in general.
    Type: String
    Default: 0.0.0.0/0

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

  NodeImageId:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.21/amazon-linux-2/recommended/image_id

Outputs:
  AppUrl:
    Value: !GetAtt LoadBalancer.DNSName

Resources:

  {% filter indent(width=2) -%}
  {% include "components/cluster.yaml" %}
  {%- endfilter %}

  BasicLifecycleRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-SnapshotPolicy-role
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: dlm.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSDataLifecycleManagerServiceRole

  BasicLifecyclePolicy:
    Type: "AWS::DLM::LifecyclePolicy"
    Properties:
      Description: !Sub "Lifecycle Policy for ${CostTagValue}-cluster"
      State: "ENABLED"
      ExecutionRoleArn: !GetAtt BasicLifecycleRole.Arn
      PolicyDetails:
        ResourceTypes:
          - "VOLUME"
        TargetTags:
          - Key: !Sub "kubernetes.io/cluster/${CostTagValue}-cluster"
            Value: "owned"
        Schedules:
          - Name: "Daily Snapshots"
            CreateRule:
              Interval: 24
              IntervalUnit: "HOURS"
              Times:
                - "10:00"
            RetainRule:
              Count: 1
            CopyTags: true

  NotificationsRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/notifications"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  CronsRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/crons"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  {%- for node in nodes %}
  {%- set node_name_escaped = node.name | regex_replace ("[^A-Za-z0-9]","specialcharacter") %}

  Repository{{ node_name_escaped }}:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub "${ContainerNamespace}/{{ node.name }}"
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}

  NodeInstanceRole{{ node_name_escaped }}:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::Region}-${CostTagValue}-cluster-{{ node.name }}-instance-role
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AutoScalingFullAccess
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
      {%- filter indent(width=6) -%}
      {% if node.is_hub is defined -%}
        {% include "components/hub_node_policy.yaml" %}
      {% else -%}
        {% include "components/user_node_policy.yaml" %}
      {% endif -%}
      {%- endfilter %}

  NodeInstanceProfile{{ node_name_escaped }}:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref NodeInstanceRole{{ node_name_escaped }}
      Roles:
        - !Ref NodeInstanceRole{{ node_name_escaped }}

  AutoScalingGroup{{ node_name_escaped }}:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref LaunchConfiguration{{ node_name_escaped }}
      MinSize: "{{ node.min_number}}"
      MaxSize: "{{ node.max_number}}"
      VPCZoneIdentifier: !Ref ActiveSubnets
      Tags:
        - Key: Name
          Value: !Sub ${CostTagValue}-cluster-instance
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster/${CostTagValue}-cluster
          Value: owned
          PropagateAtLaunch: true
        - Key: !Sub kubernetes.io/cluster/${CostTagValue}-cluster
          Value: owned
          PropagateAtLaunch: true
        - Key: k8s.io/cluster-autoscaler/enabled
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub k8s.io/cluster-autoscaler/${CostTagValue}-cluster
          Value: "true"
          PropagateAtLaunch: true
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
          PropagateAtLaunch: true

  LaunchConfiguration{{ node_name_escaped }}:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile{{ node_name_escaped }}
      ImageId: !Ref NodeImageId
      InstanceType: {{ node.instance }}
      SecurityGroups:
        - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          {% if node.is_hub is defined -%}
          /etc/eks/bootstrap.sh ${CostTagValue}-cluster --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=core,server_type=core'
          {% else -%}
          /etc/eks/bootstrap.sh ${CostTagValue}-cluster --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,server_type={{ node.name }}'
          {% endif -%}
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${CostTagValue}-cluster \
          --resource NodeGroup  \
          --region ${AWS::Region}

  {% endfor -%}

  HubProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub ${CostTagValue}-cluster-jupyterhub
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        Image: aws/codebuild/standard:3.0
        PrivilegedMode: true
      Artifacts:
        Type: CODEPIPELINE
      ServiceRole: !Sub arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-build-role
      TimeoutInMinutes: 45
      Tags:
        - Key: !Sub ${CostTagKey}
          Value: !Sub ${CostTagValue}
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub |-
              version: 0.2
              env:
                shell: bash
                secrets-manager:
                  DH_CREDS: "dockerhub/creds"
              phases:
                install:
                  runtime-versions:
                    docker: 18
                    python: 3.8
                  commands:
                    - pip3 install awscli>=1.16.158 --upgrade
                    - pip3 install boto3 --upgrade
                    - pip3 install kubernetes --upgrade
                    - pip3 install jinja2 --upgrade
                    - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/darwin/amd64/kubectl
                    - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash -s -- --version v3.7.0
                    - export HELM_HOST=127.0.0.1:44134
                    - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                    - mv /tmp/eksctl /usr/local/bin
                    - eksctl version
                pre_build:
                  commands:
                    - echo "Logging into AWS ECR..."
                    - $(aws ecr get-login --no-include-email --region ${AWS::Region})
                    - echo "Logging into Docker Hub user to get latest jupyter image..."
                    - dh_username=$(echo $DH_CREDS | cut -f1 -d' ')
                    - echo $DH_CREDS | cut -f2 -d' ' > dh.pass
                    - cat dh.pass | docker login -u $dh_username --password-stdin
                    - echo "Install helm charts and update..."
                    - helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
                    - helm repo add autoscaler https://kubernetes.github.io/autoscaler
                    - helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
                    - helm repo update
                build:
                  commands:
                    - echo "Update kubeconfig and apply config files...";
                      aws eks update-kubeconfig --name ${CostTagValue}-cluster --role-arn arn:aws:iam::${AWS::AccountId}:role/${AWS::Region}-${CostTagValue}-cluster-run-role;
                      kubectl apply -f configs/aws-auth-cm.yaml
                    - echo "Reapply storage class...";
                      kubectl delete sc gp2 || true;
                      kubectl apply -f configs/csi-sc.yaml
                    - helm upgrade aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver
                        --install
                        --version 2.4.0
                        --namespace kube-system
                        --timeout=6m0s
                        --atomic
                        --set enableVolumeScheduling=true
                        --set enableVolumeResizing=true
                        --set enableVolumeSnapshot=false
                        --set extraCreateMetadata=true
                        --set k8sTagClusterId=${CostTagValue}-cluster
                        --set extraVolumeTags.${CostTagKey}=${CostTagValue}
                    - echo "Associate IAM and OIDC for service accounts...";
                      eksctl utils associate-iam-oidc-provider --cluster ${CostTagValue}-cluster --approve
                    - echo "Get registry URI...";
                      export REGISTRY_URI='${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ContainerNamespace}'
                      echo $REGISTRY_URI
                    - echo "Build Hub Image...";
                      PrevWD=$(pwd); 
                      cd hub;
                      cp dockerfile dockerfile.build;
                      export HUB_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/hub:$HUB_IMAGE_BUILD -t $REGISTRY_URI/hub:latest .;
                      docker push $REGISTRY_URI/hub:$HUB_IMAGE_BUILD;
                      docker push $REGISTRY_URI/hub:latest;
                      cd $PrevWD;
                    - echo "Install JupyterHub cluster...";
                      base64 ./hub/helm_config.d/usr/local/share/jupyterhub/static/images/ASFLogo-Blue2.png > ASFLogo-Blue2.png.b64 &&
                      base64 ./hub/helm_config.d/usr/local/share/jupyterhub/static/images/alaska_satellite_facility_1000.png > alaska_satellite_facility_1000.png.b64 &&
                      helm upgrade jupyter jupyterhub/jupyterhub
                        --install
                        --create-namespace
                        --namespace jupyter
                        --version 1.1.3
                        --values ./hub/helm_config.yaml
                        --timeout=6m0s
                        --atomic
                        --set hub.image.name=$REGISTRY_URI/hub,hub.image.tag=$HUB_IMAGE_BUILD
                        --set hub.config.Authenticator.admin_users[0]='${AdminUserName}'
                        --set proxy.service.nodePorts.http='${NodeProxyPort}'
                        --set custom.REGISTRY_URI=$REGISTRY_URI
                        --set custom.OAUTH_JUPYTER_URL=$( [ -n '${DeploymentURL}' ] && echo '${DeploymentURL}' || echo 'https://${LoadBalancer.DNSName}' )
                        --set custom.OAUTH_DNS_NAME='${OAuthDNSName}'
                        --set custom.CLUSTER_NAME='${CostTagValue}-cluster',custom.OAUTH_POOL_NAME='${OAuthPoolName}',custom.AZ_NAME='${AWS::Region}d'
                        --set custom.COST_TAG_VALUE='${CostTagValue}'
                        --set custom.COST_TAG_KEY='${CostTagKey}'
                        --set-file hub.extraFiles.hooks.stringData='./hub/helm_config.d/hooks.py'
                        --set-file hub.extraFiles.hooks-pv.stringData='./hub/helm_config.d/hooks/pv.yaml'
                        --set-file hub.extraFiles.hooks-pvc.stringData='./hub/helm_config.d/hooks/pvc.yaml'
                        --set-file hub.extraFiles.profiles.stringData='./hub/helm_config.d/profiles.py'
                        --set-file hub.extraFiles.services.stringData='./hub/helm_config.d/services.py'
                        --set-file hub.extraFiles.lib-logout.stringData='./hub/helm_config.d/usr/local/lib/generic_with_logout.py'
                        --set-file hub.extraFiles.lib-groups.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/groups.py'
                        --set-file hub.extraFiles.lib-handlers-groups.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/handlers/groups.py'
                        --set-file hub.extraFiles.lib-handlers-init.stringData='./hub/helm_config.d/usr/local/lib/jupyterhub/handlers/__init__.py'
                        --set-file hub.extraFiles.share-images-alaska_satellite_facility_1000.binaryData='./alaska_satellite_facility_1000.png.b64'
                        --set-file hub.extraFiles.share-images-blue2.binaryData='./ASFLogo-Blue2.png.b64'
                        --set-file hub.extraFiles.share-nasa_logo.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/static/images/NASA_logo.svg'
                        --set-file hub.extraFiles.share-js-groups.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/static/js/groups.js'
                        --set-file hub.extraFiles.share-templates-groups.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/groups.html'
                        --set-file hub.extraFiles.share-templates-login.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/login.html'
                        --set-file hub.extraFiles.share-templates-pending.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/pending.html'
                        --set-file hub.extraFiles.share-templates-custom-page.stringData='./hub/helm_config.d/usr/local/share/jupyterhub/templates/custom/page.html'
                        --set-file singleuser.extraFiles.user-hooks-sar.stringData='./hub/helm_config.d/user_containers/hooks/sar.sh'
                        --set-file singleuser.extraFiles.user-hooks-sar_test.stringData='./hub/helm_config.d/user_containers/hooks/sar_test.sh'
                        --set-file singleuser.extraFiles.user-hooks-sar_dev.stringData='./hub/helm_config.d/user_containers/hooks/sar_dev.sh'
                        --set-file singleuser.extraFiles.user-hooks-no_smart_git.stringData='./hub/helm_config.d/user_containers/hooks/no_smart_git.sh'
                        --set-file singleuser.extraFiles.user-hooks-pull.stringData='./hub/helm_config.d/user_containers/hooks/pull.py'
                        --set-file singleuser.extraFiles.user-others-check_storage.stringData='./hub/helm_config.d/user_containers/others/check_storage.py'
                        --set-file singleuser.extraFiles.user-custom_magics.stringData='./hub/helm_config.d/user_containers/custom_magics/00-df.py'
                    - kubectl -n jupyter rollout status -w deployment.apps/hub
                    - echo "Associate volume provisioner with service account...";
                      kubectl create clusterrolebinding cluster-pv --clusterrole=system:persistent-volume-provisioner --serviceaccount=jupyter:hub --dry-run -o yaml | kubectl apply -f -
                    - echo "Apply CNI. This must be updated on every upgrade...";
                      curl -o aws-k8s-cni.yaml https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.9/config/v1.9/aws-k8s-cni.yaml;
                      sed -i -e 's/us-west-2/${AWS::Region}/' aws-k8s-cni.yaml;
                      kubectl apply -f aws-k8s-cni.yaml
                    - echo "Install autoscaler...";
                      helm upgrade autoscaler autoscaler/cluster-autoscaler-chart
                        --install
                        --create-namespace
                        --namespace autoscaler
                        --atomic
                        --timeout=2m0s
                        --set autoDiscovery.clusterName=${CostTagValue}-cluster,awsRegion=${AWS::Region},nodeSelector."hub\\.jupyter\\.org/node-purpose"=core
                    - echo "Install Fluent Bit for AWS Container Insights...";
                      export FluentBitHttpPort='2020';
                      export FluentBitReadFromHead='Off';
                      [[ $FluentBitReadFromHead = 'On' ]] && FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On';
                      [[ -z $FluentBitHttpPort ]] && FluentBitHttpServer='Off' || FluentBitHttpServer='On';
                      curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml | sed 's/{{cluster_name}}/${CostTagValue}-cluster/;s/{{region_name}}/${AWS::Region}/;s/{{http_server_toggle}}/"'$FluentBitHttpServer'"/;s/{{http_server_port}}/"'$FluentBitHttpPort'"/;s/{{read_from_head}}/"'$FluentBitReadFromHead'"/;s/{{read_from_tail}}/"'$FluentBitReadFromTail'"/' | kubectl apply -f -
                    - echo "Create services namespace";
                      kubectl apply -f services/namespace.yaml
                    - echo "Build Notification Image...";
                      PrevWD=$(pwd); 
                      cd services/notifications;
                      cp dockerfile dockerfile.build;
                      export NOTES_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD -t $REGISTRY_URI/notifications:latest .;
                      docker push $REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD;
                      docker push $REGISTRY_URI/notifications:latest;
                      cd $PrevWD;
                    - echo "Install notifications service...";
                      cp services/notifications/k8s/deployment.example.yaml services/notifications/k8s/deployment.yaml;
                      sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/notifications:$NOTES_IMAGE_BUILD|" services/notifications/k8s/deployment.yaml;
                      sed -i "s|ICAL_URL_PLACEHOLDER|${ICALUrl}|" services/notifications/k8s/deployment.yaml;
                      kubectl apply -f services/notifications/k8s/service.yaml;
                      kubectl apply -f services/notifications/k8s/deployment.yaml;
                      kubectl -n services rollout status deployment/notifications -w;
                      kubectl -n services scale deployment notifications --replicas=0;
                      kubectl -n services scale deployment notifications --replicas=1
                    - echo "Build Crons Image...";
                      PrevWD=$(pwd); 
                      cd services/crons;
                      cp dockerfile dockerfile.build;
                      export CRONS_IMAGE_BUILD=$(date +"%F-%H-%M-%S");
                      time docker build -f dockerfile.build -t $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD -t $REGISTRY_URI/crons:latest .;
                      docker push $REGISTRY_URI/crons:$CRONS_IMAGE_BUILD;
                      docker push $REGISTRY_URI/crons:latest;
                      cd $PrevWD;
                    - echo "Install Crons service...";
                      cp services/crons/k8s/deployment.example.yaml services/crons/k8s/deployment.yaml;
                      sed -i "s|IMAGE_PLACEHOLDER|$REGISTRY_URI/crons:$CRONS_IMAGE_BUILD|" services/crons/k8s/deployment.yaml;
                      sed -i "s|REGION_NAME_PLACEHOLDER|${AWS::Region}|" services/crons/k8s/deployment.yaml;
                      sed -i "s|COGNITO_NAME_PLACEHOLDER|${OAuthPoolName}|" services/crons/k8s/deployment.yaml;
                      sed -i "s|CLUSTER_NAME_PLACEHOLDER|${CostTagValue}-cluster|" services/crons/k8s/deployment.yaml;
                      sed -i "s|NAMESPACE_PLACEHOLDER|jupyter|" services/crons/k8s/deployment.yaml;
                      sed -i "s|DAYS_VOL_INACTIVE_TILL_TERMINATION_PLACEHOLDER|5|" services/crons/k8s/deployment.yaml;
                      kubectl apply -f services/crons/k8s/deployment.yaml;
                      kubectl -n services rollout status deployment/crons -w;
                      kubectl -n services scale deployment crons --replicas=0;
                      kubectl -n services scale deployment crons --replicas=1

      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref HubGroupLogs

  HubGroupLogs:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${CostTagValue}-cluster-hub-group"
      RetentionInDays: 30



1. Create new cluster

1. Add Banner on old OSL for scheduled downtime

1. Autorun special snapshot lifecycle manager run by setting start time to 
    `cron(30 23 * * ? *)`  # 11:30pm UTC

1. Stop all user servers in JupyterHub Admin page.

1. Disable Load Balance
    - Go to EC2 > Target Groups 
    - Click on wanted cluster LB 
    - Select Registred Targets and Deregister
   - Check for no users via no active attached user EC2 volumes

1. Export then Import Cognito users. Users will need to reset their passwords.
    ```bash
    python migrate_users_old_to_new.py \
        --old_aws_profile_name '{{ old_aws_profile_name }}' \
        --old_cognito_userpool_id '{{ old_cognito_userpool_id }}' \
        --new_aws_profile_name '{{ new_aws_profile_name }}' \
        --new_cognito_userpool_id '{{ new_cognito_userpool_id }}'
    ```

1. Check if Cognito users are properly disabled as needed. If not
    ```bash
    python sync_disabled.py \
        --old_aws_profile_name '{{ old_aws_profile_name }}' \
        --old_cognito_userpool_id '{{ old_cognito_userpool_id }}' \
        --new_aws_profile_name '{{ new_aws_profile_name }}' \
        --new_cognito_userpool_id '{{ new_cognito_userpool_id }}'
    ```

1. To speed up the password reset process
    ```bash
    python force_pass_start.py \
        --aws_profile_name '{{ new_aws_profile_name }}' \
        --cognito_userpool_id '{{ new_cognito_userpool_id }}' \
        --preauth_lambda_function_arn '{{ new_preauth_lambda_function_arn }}'
    ```

1. Once lifecycle is done, migrate snapshots.
    ```bash 
    python migrate_snapshots_old_to_new.py \
        --old_aws_profile_name '{{ old_aws_profile_name }}' \
        --old_eks_cluster_name '{{ old_eks_cluster_name }}' \
        --old_aws_region '{{ old_aws_region }}' \
        --new_aws_profile_name '{{ new_aws_profile_name }}' \
        --new_eks_cluster_name '{{ new_eks_cluster_name }}' \
        --new_aws_region '{{ new_aws_region }}' \
        --billing_tag_key '{{ billing_tag_key }}' \
        --billing_tag_value '{{ billing_tag_value }}'
    ```

1. While waiting for snapshot migration, export hub DB from old account and import hub DB to new account.
   This can only be done manually. This takes advantage of the custom kubectl shortcut.
   ```bash
    sk osl-daac
    OLD_HUB_POD=$(kubectl get pods -l component=hub --no-headers -o custom-columns=":metadata.name")
    kubectl cp $OLD_HUB_POD:/srv/jupyterhub/jupyterhub.sqlite jupyterhub.sqlite.old-daac
    sk osl-daac-test-west
    NEW_HUB_POD=$(kubectl get pods -l component=hub --no-headers -o custom-columns=":metadata.name")
    kubectl cp jupyterhub.sqlite.old-daac $NEW_HUB_POD:/srv/jupyterhub/jupyterhub.sqlite.old-daac
    kubectl exec -it $NEW_HUB_POD -- bash
    cp /srv/jupyterhub/jupyterhub.sqlite /srv/jupyterhub/jupyterhub.sqlite.deprecated
    cp /srv/jupyterhub/jupyterhub.sqlite.old-daac /srv/jupyterhub/jupyterhub.sqlite
    exit
    kubectl delete pod $NEW_HUB_POD
    ```

    - In new account Groups page create groups:
        -- SAR_1

    - If you need to revert: `cp /srv/jupyterhub/jupyterhub.sqlite.deprecated /srv/jupyterhub/jupyterhub.sqlite`

1. Update Banner on login page in new account.

1. Check New cluster
    - https://{{ new_load_balancer }}

1. Update DeploymentURL in opensarlab.yaml to 'https://{{ new_domain }}' and rebuild cluster
    - If you need to Revert: 'https://{{ new_load_balancer }}'

1. If anything goes wrong at this point and a rollback is needed, reenable old Load Balancer. Nothing else has been changed in the old DAAC.

1. Otherwise, update Puppet DNS with new Load Balancer URL for "{{ new_domain }}" from
   - from `opensarlab  IN  CNAME  {{ old_load_balancer }}.`
   - to `opensarlab  IN  CNAME  {{ new_load_balancer }}.`
   - Wait for Puppet

1. Double check cluster health

1. Reduce Old EC2s to zero.

1. After couple days, delete volumes. Daily snapshots will have ran.

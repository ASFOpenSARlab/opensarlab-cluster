AWSTemplateFormatVersion: 2010-09-09

Parameters:

  VpcId:
    Description: The VPC of the worker instances
    Type: AWS::EC2::VPC::Id

  Subnets:
    Description: The subnets where workers can be created.
    Type: List<AWS::EC2::Subnet::Id>

  ActiveSubnets:
    Description: The subnets actually used by resources in the cluster. Typically only one (e.g., subnet for AZ -d).
    Type: List<AWS::EC2::Subnet::Id>

  CodeBuildServiceRoleArn:
    Description: Role externally created to give Code Build permission to use AWS resources.
    Type: String

  AdminUserName:
    Description: User name of main admin. This name is also whitelisted. Other users will need to be added via the Jupyter Hub admin console. This name MUST be a valid Earthdata user.
    Type: String

  JupyterHubURL:
    Description: Jupyterhub URL used by AWS Cognito authentication. If not given, the default is the load balancer URL
    Type: String
    Default: ''

  OAuthDnsName:
    Description: AWS Cognito authentication DNS name
    Type: String
    Default: ''

  OAuthClientId:
    Description: AWS Cognito authentication client id.
    Type: String

  OAuthClientSecret:
    Description: AWS Cognito authentication client secret.
    Type: String
    NoEcho: True

  ImageName:
    Description: Name of ECR docker image.
    Type: String
    Default: 553778890976.dkr.ecr.us-east-1.amazonaws.com/asf-franz-labs

  ImageTag:
    Description: Tag of ECR docker image.
    Type: String

  NodeImageId:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.14/amazon-linux-2/recommended/image_id

  NodeInstanceTypeGPU:
    Description: EC2 instance type for the node instances.
    Type: String
    Default: g4dn.2xlarge

  NodeInstanceTypeCPU:
    Description: EC2 instance type for the node instances.
    Type: String
    Default: m5.2xlarge

  NodeAutoScalingGroupMinSizeCPU:
    Description: Minimum size of Node Group ASG.
    Type: Number
    Default: 2

  NodeAutoScalingGroupMaxSizeCPU:
    Description: Maximum size of Node Group ASG. Set to at least 1 greater than NodeAutoScalingGroupDesiredCapacityCPU.
    Type: Number
    Default: 8

  NodeAutoScalingGroupDesiredCapacityCPU:
    Description: Desired capacity of Node Group ASG.
    Type: Number
    Default: 2



  NodeVolumeSize:
    Description: Node volume size (GB)
    Type: Number
    Default: 500

  NodeAccessKeyId:
    Description: The access key to allow aws cli usage. Specific AWS resource access is handled by the user attached to the key.
    Type: String
    Default: ""

  NodeSecretKey:
    Description: The secret key to allow aws cli usage. Specific AWS resource access is handled by the user attached to the key.
    Type: String
    Default: ""
    NoEcho: true

  LoadBalancerCidrBlock:
    Description: The range of allowed IPv4 addresses for the load balancer. This only firewalls the load balancer URL and not the cluster in general.
    Type: String
    Default: 0.0.0.0/0

  CertificateArn:
    Description: The ARN of the SSL certificate attached to the load balancer.
    Type: String
    Default: arn:aws:acm:us-east-1:553778890976:certificate/10791780-75d2-4ef0-bfbc-2c074e94b92b

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

  HubAWSId:
    Description: The access key to allow the hub to work with Boto3.
    Type: String
    Default: ""

  HubAWSSecret:
    Description: The secret key to allow the hub to work with Boto3.
    Type: String
    Default: ""
    NoEcho: true

Outputs:

  AppUrl:
    Value: !GetAtt LoadBalancer.DNSName

Resources:

  ClusterRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-cluster
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: eks.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      - arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      - arn:aws:iam::aws:policy/AutoScalingFullAccess

  ClusterSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-cluster
      GroupDescription: !Sub Security group for the ${AWS::StackName} EKS cluster
      VpcId: !Ref VpcId

  Cluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name: !Ref AWS::StackName
      RoleArn: !GetAtt ClusterRole.Arn
      ResourcesVpcConfig:
        SubnetIds: !Ref Subnets
        SecurityGroupIds:
        - !GetAtt ClusterSecurityGroup.GroupId

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-instance
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      - arn:aws:iam::aws:policy/AutoScalingFullAccess
      - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

  NodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref NodeInstanceRole
      Roles:
      - !Ref NodeInstanceRole

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-instance
      GroupDescription: !Sub Security group for instances in the ${Cluster} EKS cluster
      VpcId: !Ref VpcId

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow instances to communicate with each other
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: -1
      FromPort: 0
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromLoadBalancer:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the load balancer
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt LoadBalancerSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: !Ref NodeProxyPort
      ToPort: !Ref NodeProxyPort

  ControlPlaneEgressToNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroupEgress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow the cluster control plane to communicate with worker Kubelet and pods
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneOn443Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow pods running extension API servers on port 443 to receive communication from cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ControlPlaneEgressToNodeSecurityGroupOn443:
    Type: AWS::EC2::SecurityGroupEgress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow the cluster control plane to communicate with pods running extension API servers on port 443
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ClusterControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow pods to communicate with the cluster API Server
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      ToPort: 443
      FromPort: 443

  AutoScalingGroupCore:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: 1
      LaunchConfigurationName: !Ref LaunchConfigurationCore
      MinSize: 1
      MaxSize: 2
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: true
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: true
        PropagateAtLaunch: true
      - Key: "k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose"
        Value: "core"
        PropagateAtLaunch: true

  LaunchConfigurationCore:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: "t3.micro"
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster}
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupGPU:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: 0
      LaunchConfigurationName: !Ref LaunchConfigurationGPU
      MinSize: 0
      MaxSize: 5
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: true
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: true
        PropagateAtLaunch: true
      - Key: "k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose"
        Value: "user"
        PropagateAtLaunch: true
      - Key: "k8s.io/cluster-autoscaler/node-template/label/EC2hasGPU"
        Value: true
        PropagateAtLaunch: true
      - Key: "k8s.io/cluster-autoscaler/node-template/label/k8s.amazonaws.com/accelerator"
        Value: "nvidia-tesla-k80"
        PropagateAtLaunch: true

  LaunchConfigurationGPU:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceTypeGPU
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster}
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupCPU:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: !Ref NodeAutoScalingGroupDesiredCapacityCPU
      LaunchConfigurationName: !Ref LaunchConfigurationCPU
      MinSize: !Ref NodeAutoScalingGroupMinSizeCPU
      MaxSize: !Ref NodeAutoScalingGroupMaxSizeCPU
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: true
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: true
        PropagateAtLaunch: true
      - Key: "k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose"
        Value: "user"
        PropagateAtLaunch: true

  LaunchConfigurationCPU:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceTypeCPU
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster}
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: !Ref AWS::StackName
      VpcId: !Ref VpcId
      Protocol: HTTP
      Port: !Ref NodeProxyPort
      HealthCheckPath: /hub/login
      HealthCheckIntervalSeconds: 120
      HealthyThresholdCount: 2

  LoadBalancerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-load-balancer
      GroupDescription: !Sub Security group for ${AWS::StackName} load balancer
      VpcId: !Ref VpcId
      SecurityGroupIngress:
      - CidrIp: !Ref LoadBalancerCidrBlock
        IpProtocol: tcp
        FromPort: 80
        ToPort: 80
      - CidrIp: !Ref LoadBalancerCidrBlock
        IpProtocol: tcp
        FromPort: 443
        ToPort: 443

  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: !Ref AWS::StackName
      Subnets: !Ref Subnets
      SecurityGroups:
      - !GetAtt LoadBalancerSecurityGroup.GroupId

  HttpListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTP
      Port: 80
      DefaultActions:
      - Type: redirect
        RedirectConfig:
          StatusCode: HTTP_301
          Protocol: HTTPS
          Port: 443

  HttpsListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTPS
      Port: 443
      Certificates:
      - CertificateArn: !Ref CertificateArn
      DefaultActions:
      - Type: forward
        TargetGroupArn: !Ref TargetGroup

  BasicLifecyclePolicy:
    Type: "AWS::DLM::LifecyclePolicy"
    Properties:
      Description: !Sub "Lifecycle Policy for ${Cluster}"
      State: "ENABLED"
      ExecutionRoleArn: "arn:aws:iam::553778890976:role/service-role/AWSDataLifecycleManagerDefaultRole"
      PolicyDetails:
        ResourceTypes:
          - "VOLUME"
        TargetTags:
          -
            Key: !Sub "kubernetes.io/cluster/${Cluster}"
            Value: "owned"
        Schedules:
          -
            Name: "Daily Snapshots"
            CreateRule:
              Interval: 24
              IntervalUnit: "HOURS"
              Times:
                - "10:00"
            RetainRule:
              Count: 1
            CopyTags: true

  ProjectLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${AWS::StackName}"
      RetentionInDays: 30

  Project:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Ref AWS::StackName
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        Image: aws/codebuild/standard:3.0
        PrivilegedMode: True
      Artifacts:
        Type: CODEPIPELINE
      ServiceRole: !Ref CodeBuildServiceRoleArn
      TimeoutInMinutes: 45
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub |-
          version: 0.2
          phases:
            install:
              runtime-versions:
                docker: 18
                python: 3.8
              commands:
                - pip3 install awscli>=1.16.158 --upgrade
            pre_build:
              commands:
                - echo "logging in to AWS ECR..."
                - $(aws ecr get-login --no-include-email --region us-east-1)
            build:
              commands:
                - HUB_IMAGE_TAG=$(date +"%F-%H-%M-%S")
                - docker build -t 553778890976.dkr.ecr.us-east-1.amazonaws.com/jupyter-hub:$HUB_IMAGE_TAG -f hub/dockerfile hub/
                - docker push 553778890976.dkr.ecr.us-east-1.amazonaws.com/jupyter-hub:$HUB_IMAGE_TAG
                - wget --no-verbose https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/kubectl
                - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash
                - aws eks update-kubeconfig --name ${Cluster}
                - sed -i "s|INSTANCE_ROLE_ARN|${NodeInstanceRole.Arn}|" aws-auth-cm.yaml
                - kubectl apply -f aws-auth-cm.yaml
                - kubectl apply -f ebs-sc.yaml
                - helm init --client-only
                - helm plugin install https://github.com/rimusz/helm-tiller
                - helm tiller start-ci
                - export HELM_HOST=127.0.0.1:44134
                - helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
                - helm repo update
                - helm upgrade jupyter jupyterhub/jupyterhub --install --namespace jupyter  --version 0.9.0-beta.4  --values helm_config.yaml --timeout=1800
                    --set auth.admin.users[0]=${AdminUserName}
                    --set proxy.secretToken=$(openssl rand -hex 32),proxy.service.nodePorts.http='${NodeProxyPort}'
                    --set singleuser.extraEnv.AWS_ACCESS_KEY_ID='${NodeAccessKeyId}',singleuser.extraEnv.AWS_SECRET_ACCESS_KEY='${NodeSecretKey}',singleuser.image.name='${ImageName}',singleuser.image.tag='${ImageTag}'
                    --set custom.OAUTH_CLIENT_ID='${OAuthClientId}',custom.OAUTH_CLIENT_SECRET='${OAuthClientSecret}'
                    --set custom.OAUTH_JUPYTER_URL=$( [ -n '${JupyterHubURL}' ] && echo '${JupyterHubURL}' || echo 'https://${LoadBalancer.DNSName}' )
                    --set custom.OAUTH_DNS_NAME='${OAuthDnsName}'
                    --set custom.HUB_ACCESS_KEY_ID='${HubAWSId}',custom.HUB_SECRET_ACCESS_KEY='${HubAWSSecret}'
                    --set custom.CLUSTER_NAME=${Cluster},custom.AZ_NAME=${AWS::Region}d
                    --set custom.VOLUME_SIZE='${NodeVolumeSize}'
                    --set hub.image.tag=$HUB_IMAGE_TAG
                - kubectl -n jupyter rollout status -w deployment.apps/hub
                - kubectl --logtostderr=true -n jupyter patch deployment hub -p '{"spec":{"template":{"spec":{"containers":[{"name":"hub","command":["sh","-c","sudo /usr/sbin/service cron start && jupyterhub --config /etc/jupyterhub/jupyterhub_config.py --upgrade-db"],"securityContext":{"allowPrivilegeEscalation":"true"}}]}}}}' || true
                - kubectl create clusterrolebinding cluster-pv --clusterrole=system:persistent-volume-provisioner --serviceaccount=jupyter:hub --dry-run -o yaml | kubectl apply -f -
                - helm repo update
                - helm fetch stable/cluster-autoscaler
                - tar -zxf cluster-autoscaler*
                - helm upgrade autoscaler stable/cluster-autoscaler --install --namespace autoscaler --values cluster-autoscaler*/values.yaml --timeout=120
                    --set autoDiscovery.clusterName=${Cluster},awsRegion=${AWS::Region},sslCertPath='/etc/kubernetes/pki/ca.crt',rbac.create=True
                - curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/master/k8s-yaml-templates/quickstart/cwagent-fluentd-quickstart.yaml | sed "s/{{cluster_name}}/${Cluster}/;s/{{region_name}}/${AWS::Region}/" | kubectl apply -f -
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref ProjectLogGroup

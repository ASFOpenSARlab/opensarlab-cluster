AWSTemplateFormatVersion: 2010-09-09

Parameters:

  VpcId:
    Description: The VPC of the worker instances
    Type: AWS::EC2::VPC::Id

  Subnets:
    Description: The subnets where workers can be created.
    Type: List<AWS::EC2::Subnet::Id>

  CodeBuildServiceRoleArn:
    Description: Role externally created to give Code Build permission to use AWS resources.
    Type: String

  AdminUserName:
    Description: User name of main admin. This name is also whitelisted. Other users will need to be added via the Jupyter Hub admin console. This name MUST be a valid Earthdata user.
    Type: String

  JupyterHubURL:
    Description: Jupyterhub URL used by AWS Cognito authentication. If not given, the default is the load balancer URL
    Type: String
    Default: ''

  OAuthDnsName:
    Description: AWS Cognito authentication DNS name
    Type: String
    Default: ''

  OAuthClientId:
    Description: AWS Cognito authentication client id.
    Type: String

  OAuthClientSecret:
    Description: AWS Cognito authentication client secret.
    Type: String
    NoEcho: True

  ImageName:
    Description: Name of ECR docker image.
    Type: String
    Default: 553778890976.dkr.ecr.us-east-1.amazonaws.com/asf-franz-labs

  ImageTag:
    Description: Tag of ECR docker image.
    Type: String

  NodeImageId:
    Description: AMI id for the node instances.
    Type: AWS::EC2::Image::Id
    Default: ami-0eeeef929db40543c

  NodeInstanceType:
    Description: EC2 instance type for the node instances.
    Type: String
    Default: m5.2xlarge

  NodeAutoScalingGroupMinSize:
    Description: Minimum size of Node Group ASG.
    Type: Number
    Default: 1

  NodeAutoScalingGroupMaxSize:
    Description: Maximum size of Node Group ASG. Set to at least 1 greater than NodeAutoScalingGroupDesiredCapacity.
    Type: Number
    Default: 4

  NodeAutoScalingGroupDesiredCapacity:
    Description: Desired capacity of Node Group ASG.
    Type: Number
    Default: 2

  NodeVolumeSize:
    Description: Node volume size (GB)
    Type: Number
    Default: 100

  NodeAccessKeyId:
    Description: The access key to allow aws cli usage. Specific AWS resource access is handled by the user attached to the key.
    Type: String
    Default: ""

  NodeSecretKey:
    Description: The secret key to allow aws cli usage. Specific AWS resource access is handled by the user attached to the key.
    Type: String
    Default: ""
    NoEcho: true

  LoadBalancerCidrBlock:
    Description: The range of allowed IPv4 addresses for the load balancer. This only firewalls the load balancer URL and not the cluster in general.
    Type: String
    Default: 0.0.0.0/0

  CertificateArn:
    Description: The ARN of the SSL certificate attached to the load balancer.
    Type: String

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

Outputs:

  AppUrl:
    Value: !GetAtt LoadBalancer.DNSName

Resources:

  ClusterRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-cluster
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: eks.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      - arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      - arn:aws:iam::aws:policy/AutoScalingFullAccess

  ClusterSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-cluster
      GroupDescription: !Sub Security group for the ${AWS::StackName} EKS cluster
      VpcId: !Ref VpcId

  Cluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name: !Ref AWS::StackName
      RoleArn: !GetAtt ClusterRole.Arn
      ResourcesVpcConfig:
        SubnetIds: !Ref Subnets
        SecurityGroupIds:
        - !GetAtt ClusterSecurityGroup.GroupId

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-instance
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      - arn:aws:iam::aws:policy/AutoScalingFullAccess
      - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

  NodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref NodeInstanceRole
      Roles:
      - !Ref NodeInstanceRole

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-instance
      GroupDescription: !Sub Security group for instances in the ${Cluster} EKS cluster
      VpcId: !Ref VpcId

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow instances to communicate with each other
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: -1
      FromPort: 0
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromLoadBalancer:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the load balancer
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt LoadBalancerSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: !Ref NodeProxyPort
      ToPort: !Ref NodeProxyPort

  ControlPlaneEgressToNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroupEgress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow the cluster control plane to communicate with worker Kubelet and pods
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneOn443Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow pods running extension API servers on port 443 to receive communication from cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ControlPlaneEgressToNodeSecurityGroupOn443:
    Type: AWS::EC2::SecurityGroupEgress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow the cluster control plane to communicate with pods running extension API servers on port 443
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ClusterControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow pods to communicate with the cluster API Server
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      ToPort: 443
      FromPort: 443

  AutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: !Ref NodeAutoScalingGroupDesiredCapacity
      LaunchConfigurationName: !Ref LaunchConfiguration
      MinSize: !Ref NodeAutoScalingGroupMinSize
      MaxSize: !Ref NodeAutoScalingGroupMaxSize
      VPCZoneIdentifier: !Ref Subnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: true
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: true
        PropagateAtLaunch: true

  LaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceType
      SecurityGroups:
      - !Ref NodeSecurityGroup
      BlockDeviceMappings:
      - DeviceName: /dev/xvda
        Ebs:
          VolumeSize: !Ref NodeVolumeSize
          VolumeType: gp2
          DeleteOnTermination: true
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster}
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: !Ref AWS::StackName
      VpcId: !Ref VpcId
      Protocol: HTTP
      Port: !Ref NodeProxyPort
      HealthCheckPath: /hub/login
      HealthCheckIntervalSeconds: 120
      HealthyThresholdCount: 2

  LoadBalancerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-load-balancer
      GroupDescription: !Sub Security group for ${AWS::StackName} load balancer
      VpcId: !Ref VpcId
      SecurityGroupIngress:
      - CidrIp: !Ref LoadBalancerCidrBlock
        IpProtocol: tcp
        FromPort: 80
        ToPort: 80
      - CidrIp: !Ref LoadBalancerCidrBlock
        IpProtocol: tcp
        FromPort: 443
        ToPort: 443

  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: !Ref AWS::StackName
      Subnets: !Ref Subnets
      SecurityGroups:
      - !GetAtt LoadBalancerSecurityGroup.GroupId

  HttpListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTP
      Port: 80
      DefaultActions:
      - Type: redirect
        RedirectConfig:
          StatusCode: HTTP_301
          Protocol: HTTPS
          Port: 443

  HttpsListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTPS
      Port: 443
      Certificates:
      - CertificateArn: !Ref CertificateArn
      DefaultActions:
      - Type: forward
        TargetGroupArn: !Ref TargetGroup

  ProjectLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${AWS::StackName}"
      RetentionInDays: 30

  Project:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Ref AWS::StackName
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        Image: aws/codebuild/standard:1.0
      Artifacts:
        Type: CODEPIPELINE
      ServiceRole: !Ref CodeBuildServiceRoleArn
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub |-
          version: 0.2
          phases:
            build:
              commands:
                - pip3 install awscli>=1.16.158 --upgrade
                - wget --no-verbose https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/kubectl
                - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash
                - aws eks update-kubeconfig --name ${Cluster}
                - sed -i "s|INSTANCE_ROLE_ARN|${NodeInstanceRole.Arn}|" aws-auth-cm.yaml
                - kubectl apply -f aws-auth-cm.yaml
                - helm init --client-only
                - helm plugin install https://github.com/rimusz/helm-tiller
                - helm tiller start-ci
                - export HELM_HOST=127.0.0.1:44134
                - kubectl -n jupyter apply -f hub_efs_claim.yaml
                - helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
                - helm repo update
                - helm upgrade jupyter jupyterhub/jupyterhub --install --namespace jupyter --version 0.8.2 --values helm_config.yaml --timeout=300
                    --set auth.admin.users[0]=${AdminUserName}
                    --set proxy.secretToken=$(openssl rand -hex 32),proxy.service.nodePorts.http='${NodeProxyPort}'
                    --set singleuser.extraEnv.AWS_ACCESS_KEY_ID='${NodeAccessKeyId}',singleuser.extraEnv.AWS_SECRET_ACCESS_KEY='${NodeSecretKey}',singleuser.image.name='${ImageName}',singleuser.image.tag='${ImageTag}'
                    --set custom.OAUTH_CLIENT_ID='${OAuthClientId}',custom.OAUTH_CLIENT_SECRET='${OAuthClientSecret}'
                    --set custom.OAUTH_JUPYTER_URL=$( [ -n '${JupyterHubURL}' ] && echo '${JupyterHubURL}' || echo 'https://${LoadBalancer.DNSName}' )
                    --set custom.OAUTH_DNS_NAME='${OAuthDnsName}'
                - kubectl -n jupyter rollout status -w deployment.apps/hub
                - helm upgrade efs stable/efs-provisioner --install --namespace efs --set efsProvisioner.efsFileSystemId=fs-c0386723 --set efsProvisioner.awsRegion=${AWS::Region}
                    --set efsProvisioner.path=/ --set efsProvisioner.storageClass.reclaimPolicy=Retain
                - hub_name=$(kubectl get pod -n jupyter -l app=jupyterhub,component=hub --output=jsonpath={.items..metadata.name})
                - kubectl cp generic_with_logout.py jupyter/$hub_name:/srv/jupyterhub/generic_with_logout.py
                - kubectl cp login.html jupyter/$hub_name:/srv/jupyterhub/login.html
                - kubectl cp pending.html jupyter/$hub_name:/srv/jupyterhub/pending.html
                - kubectl -n jupyter delete pod $hub_name
                - helm repo update
                - helm fetch stable/cluster-autoscaler
                - tar -zxf cluster-autoscaler*
                - helm upgrade autoscaler stable/cluster-autoscaler --install --namespace autoscaler --values cluster-autoscaler*/values.yaml --timeout=120
                    --set autoDiscovery.clusterName=${Cluster},awsRegion=${AWS::Region},sslCertPath='/etc/kubernetes/pki/ca.crt',rbac.create=True
                - curl -O https://s3.amazonaws.com/cloudwatch-agent-k8s-yamls/kubernetes-monitoring/cloudwatch-namespace.yaml
                - kubectl apply -f cloudwatch-namespace.yaml
                - curl -O https://s3.amazonaws.com/cloudwatch-agent-k8s-yamls/kubernetes-monitoring/cwagent-serviceaccount.yaml
                - kubectl apply -f cwagent-serviceaccount.yaml
                - kubectl apply -f cwagent-configmap.yaml
                - curl -O https://s3.amazonaws.com/cloudwatch-agent-k8s-yamls/kubernetes-monitoring/cwagent-daemonset.yaml
                - kubectl apply -f cwagent-daemonset.yaml
                - curl -O https://s3.amazonaws.com/cloudwatch-agent-k8s-yamls/fluentd/fluentd.yml
                - kubectl create configmap cluster-info --from-literal=cluster.name=${Cluster} --from-literal=logs.region=${AWS::Region} -n amazon-cloudwatch --dry-run -o yaml | kubectl apply -f -
                - kubectl apply -f fluentd.yml
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref ProjectLogGroup
      TimeoutInMinutes: 10

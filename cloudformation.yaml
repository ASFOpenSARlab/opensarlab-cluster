AWSTemplateFormatVersion: 2010-09-09

Parameters:

  LoadBalancerCidrBlock:
    Description: The range of allowed IPv4 addresses for the load balancer. This only firewalls the load balancer URL and not the cluster in general.
    Type: String
    Default: 0.0.0.0/0

  CertificateArn:
    Description: The ARN of the SSL certificate attached to the load balancer.
    Type: String
    Default: arn:aws:acm:us-east-1:553778890976:certificate/21d7b22a-86a3-4727-ab3a-2579a58d2d46

  NodeProxyPort:
    Description: The port of the hub proxy service opened to the load balancer.
    Type: Number
    Default: 30052

  VpcId:
    Description: The VPC of the worker instances
    Type: AWS::EC2::VPC::Id

  Subnets:
    Description: The subnets where workers can be created.
    Type: List<AWS::EC2::Subnet::Id>

  ActiveSubnets:
    Description: The subnets actually used by resources in the cluster. Typically only one (e.g., subnet for AZ -d).
    Type: List<AWS::EC2::Subnet::Id>

  AdminUserName:
    Description: User name of main admin. This name is also whitelisted. Other users will need to be added via the Jupyter Hub admin console. This name MUST be a valid Earthdata user.
    Type: String

  JupyterHubURL:
    Description: Jupyterhub URL used by AWS Cognito authentication. If not given, the default is the load balancer URL
    Type: String
    Default: ''

  OAuthDnsName:
    Description: AWS Cognito authentication DNS name
    Type: String
    Default: ''

  OAuthClientId:
    Description: AWS Cognito authentication client id.
    Type: String

  OAuthClientSecret:
    Description: AWS Cognito authentication client secret.
    Type: String
    NoEcho: True

  ImageName:
    Description: Name of ECR docker image for user's server.
    Type: String
    Default: 553778890976.dkr.ecr.us-east-1.amazonaws.com/opensarlab/general_cpu

  ImageTag:
    Description: Tag of ECR docker image for user's server.
    Type: String

  HubImageName:
    Description: Name of ECR docker image for hub pod
    Type: String
    Default: 553778890976.dkr.ecr.us-east-1.amazonaws.com/jupyter-hub

  NodeImageIdGPU:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.17/amazon-linux-2-gpu/recommended/image_id

  NodeImageIdCPU:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.17/amazon-linux-2/recommended/image_id

  NodeImageIdCPULarge:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.17/amazon-linux-2/recommended/image_id

  NodeImageIdCore:
    Description: AMI id for the node instances of K8s
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/eks/optimized-ami/1.17/amazon-linux-2/recommended/image_id

  NodeInstanceTypeCore:
    Description: EC2 instance type for the node Core instances.
    Type: String
    Default: t3a.medium

  NodeInstanceTypeGPU:
    Description: EC2 instance type for the node GPU instances.
    Type: String
    Default: g4dn.2xlarge

  NodeInstanceTypeCPU:
    Description: EC2 instance type for the node CPU instances.
    Type: String
    Default: m5.2xlarge

  NodeInstanceTypeCPULarge:
    Description: EC2 instance type for the large node CPU instances.
    Type: String
    Default: m5.8xlarge

  NodeAutoScalingGroupMinSizeCPU:
    Description: Minimum size of Node Group ASG.
    Type: Number
    Default: 2

  NodeAutoScalingGroupMaxSizeCPU:
    Description: Maximum size of Node Group ASG. Set to at least 1 greater than NodeAutoScalingGroupDesiredCapacityCPU.
    Type: Number
    Default: 8

  NodeAutoScalingGroupDesiredCapacityCPU:
    Description: Desired capacity of Node Group ASG.
    Type: Number
    Default: 2

  NodeAutoScalingGroupMinSizeCPULarge:
    Description: Minimum size of Node Group ASG.
    Type: Number
    Default: 0

  NodeAutoScalingGroupMaxSizeCPULarge:
    Description: Maximum size of Node Group ASG. Set to at least 1 greater than NodeAutoScalingGroupDesiredCapacityCPU.
    Type: Number
    Default: 25

  NodeAutoScalingGroupDesiredCapacityCPULarge:
    Description: Desired capacity of Node Group ASG.
    Type: Number
    Default: 0

Outputs:

  AppUrl:
    Value: !GetAtt LoadBalancer.DNSName

Resources:

  ClusterRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-cluster
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: eks.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      - arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      - arn:aws:iam::aws:policy/AutoScalingFullAccess

  ClusterSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-cluster
      GroupDescription: !Sub Security group for the ${AWS::StackName} EKS cluster
      VpcId: !Ref VpcId

  Cluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name: !Ref AWS::StackName
      RoleArn: !GetAtt ClusterRole.Arn
      ResourcesVpcConfig:
        SubnetIds: !Ref Subnets
        SecurityGroupIds:
        - !GetAtt ClusterSecurityGroup.GroupId

  UserAccessRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-access
      AssumeRolePolicyDocument:
        Statement:
          Effect: Deny
          Principal:
            AWS: !Sub arn:aws:iam::${AWS::AccountId}:root
          Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub ${AWS::StackName}-access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "eks:*"
                Resource: '*'

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-instance
      AssumeRolePolicyDocument:
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      - arn:aws:iam::aws:policy/AutoScalingFullAccess
      - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

  NodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Ref NodeInstanceRole
      Roles:
      - !Ref NodeInstanceRole

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-instance
      GroupDescription: !Sub Security group for instances in the ${Cluster} EKS cluster
      VpcId: !Ref VpcId

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow instances to communicate with each other
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: "-1"
      FromPort: 0
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromLoadBalancer:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the load balancer
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt LoadBalancerSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: !Ref NodeProxyPort
      ToPort: !Ref NodeProxyPort

  ControlPlaneEgressToNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      Description: Allow the cluster control plane to communicate with worker Kubelet and pods
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneOn443Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow pods running extension API servers on port 443 to receive communication from cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !GetAtt ClusterSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ControlPlaneEgressToNodeSecurityGroupOn443:
    Type: AWS::EC2::SecurityGroupEgress
    Properties:
      Description: Allow the cluster control plane to communicate with pods running extension API servers on port 443
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 443
      ToPort: 443

  ClusterControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      Description: Allow pods to communicate with the cluster API Server
      GroupId: !GetAtt ClusterSecurityGroup.GroupId
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      ToPort: 443
      FromPort: 443

  AutoScalingGroupCore:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: "1"
      LaunchConfigurationName: !Ref LaunchConfigurationCore
      MinSize: "1"
      MaxSize: "2"
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: "true"
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: "true"
        PropagateAtLaunch: true

  LaunchConfigurationCore:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageIdCore
      InstanceType: !Ref NodeInstanceTypeCore
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=core,server_type=core'
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupGPU:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: "0"
      LaunchConfigurationName: !Ref LaunchConfigurationGPU
      MinSize: "0"
      MaxSize: "5"
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: "true"
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: "true"
        PropagateAtLaunch: true

  LaunchConfigurationGPU:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageIdGPU
      InstanceType: !Ref NodeInstanceTypeGPU
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,k8s.amazonaws.com/accelerator=nvidia-tesla-k80,server_type=general_gpu'
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupCPU:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: !Ref NodeAutoScalingGroupDesiredCapacityCPU
      LaunchConfigurationName: !Ref LaunchConfigurationCPU
      MinSize: !Ref NodeAutoScalingGroupMinSizeCPU
      MaxSize: !Ref NodeAutoScalingGroupMaxSizeCPU
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: "true"
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: "true"
        PropagateAtLaunch: true

  LaunchConfigurationCPU:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageIdCPU
      InstanceType: !Ref NodeInstanceTypeCPU
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,server_type=general_cpu'
          /opt/aws/bin/cfn-signal --exit-code $? \
          --stack  ${AWS::StackName} \
          --resource NodeGroup  \
          --region ${AWS::Region}

  AutoScalingGroupCPULarge:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: !Ref NodeAutoScalingGroupDesiredCapacityCPULarge
      LaunchConfigurationName: !Ref LaunchConfigurationCPULarge
      MinSize: !Ref NodeAutoScalingGroupMinSizeCPULarge
      MaxSize: !Ref NodeAutoScalingGroupMaxSizeCPULarge
      VPCZoneIdentifier: !Ref ActiveSubnets
      TargetGroupARNs:
      - !Ref TargetGroup
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-instance
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: !Sub kubernetes.io/cluster/${Cluster}
        Value: owned
        PropagateAtLaunch: true
      - Key: k8s.io/cluster-autoscaler/enabled
        Value: "true"
        PropagateAtLaunch: true
      - Key: !Sub k8s.io/cluster-autoscaler/${Cluster}
        Value: "true"
        PropagateAtLaunch: true

  LaunchConfigurationCPULarge:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: true
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageIdCPULarge
      InstanceType: !Ref NodeInstanceTypeCPULarge
      SecurityGroups:
      - !Ref NodeSecurityGroup
      UserData:
        Fn::Base64: !Sub |
            #!/bin/bash
            set -o xtrace
            /etc/eks/bootstrap.sh ${Cluster} --kubelet-extra-args '--node-labels=hub.jupyter.org/node-purpose=user,server_type=general_cpu_large'
            /opt/aws/bin/cfn-signal --exit-code $? \
            --stack  ${AWS::StackName} \
            --resource NodeGroup  \
            --region ${AWS::Region}

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: !Ref AWS::StackName
      VpcId: !Ref VpcId
      Protocol: HTTP
      Port: !Ref NodeProxyPort
      HealthCheckPath: /hub/health
      HealthCheckIntervalSeconds: 120
      HealthyThresholdCount: 2

  LoadBalancerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName}-load-balancer
      GroupDescription: !Sub Security group for ${AWS::StackName} load balancer
      VpcId: !Ref VpcId
      SecurityGroupIngress:
      - CidrIp: !Ref LoadBalancerCidrBlock
        IpProtocol: tcp
        FromPort: 80
        ToPort: 80
      - CidrIp: !Ref LoadBalancerCidrBlock
        IpProtocol: tcp
        FromPort: 443
        ToPort: 443

  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: !Ref AWS::StackName
      Subnets: !Ref Subnets
      SecurityGroups:
      - !GetAtt LoadBalancerSecurityGroup.GroupId

  HttpListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTP
      Port: 80
      DefaultActions:
      - Type: redirect
        RedirectConfig:
          StatusCode: HTTP_301
          Protocol: HTTPS
          Port: "443"

  HttpsListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Protocol: HTTPS
      Port: 443
      Certificates:
      - CertificateArn: !Ref CertificateArn
      DefaultActions:
      - Type: forward
        TargetGroupArn: !Ref TargetGroup

  BasicLifecyclePolicy:
    Type: "AWS::DLM::LifecyclePolicy"
    Properties:
      Description: !Sub "Lifecycle Policy for ${Cluster}"
      State: "ENABLED"
      ExecutionRoleArn: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/AWSDataLifecycleManagerDefaultRole"
      PolicyDetails:
        ResourceTypes:
          - "VOLUME"
        TargetTags:
          -
            Key: !Sub "kubernetes.io/cluster/${Cluster}"
            Value: "owned"
        Schedules:
          -
            Name: "Daily Snapshots"
            CreateRule:
              Interval: 24
              IntervalUnit: "HOURS"
              Times:
                - "10:00"
            RetainRule:
              Count: 1
            CopyTags: true

  ProjectLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${AWS::StackName}"
      RetentionInDays: 30

  CodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-hub-build
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: 
                - codebuild.amazonaws.com
                - cloudformation.amazonaws.com
                - codepipeline.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub ${AWS::StackName}-hub-build
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "codestar-connections:*"
                  - "s3:*"
                  - "dlm:*"
                  - "logs:*"
                  - "cloudformation:*"
                  - "elasticloadbalancing:*"
                  - "autoscaling:*"
                  - "codebuild:*"
                  - "iam:*"
                  - "secretsmanager:*"
                  - "ssm:*"
                  - "ecr:*"
                  - "ec2:*"
                  - "eks:*"
                Resource: '*'

  JupyterImageSecret:
    Type: AWS::SecretsManager::Secret
    Properties: 
      Description: 'Docker Hub creds to grab the Jupyter base image'
      Name: !Sub ${AWS::StackName}-dockerhub/creds
      SecretString: 'USERNAME PASSWORD'
      Tags: 
        - Key: 'StackName'
          Value: !Sub ${AWS::StackName}

  Project:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Ref AWS::StackName
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        Image: aws/codebuild/standard:3.0
        PrivilegedMode: True
      Artifacts:
        Type: CODEPIPELINE
      ServiceRole: !Ref CodeBuildServiceRole
      TimeoutInMinutes: 45
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub |-
          version: 0.2
          phases:
            install:
              runtime-versions:
                docker: 18
                python: 3.8
              commands:
                - pip3 install awscli>=1.16.158 --upgrade
                - pip3 install boto3 --upgrade
                - pip3 install kubernetes
                - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.17.11/2020-09-18/bin/linux/amd64/kubectl
                - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
                - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                - mv /tmp/eksctl /usr/local/bin
                - eksctl version
            pre_build:
              commands:
                - echo "Logging into AWS ECR..."
                - $(aws ecr get-login --no-include-email --region ${AWS::Region})
                - echo "Logging into Docker Hub user to get latest jupyter image..."
                - dh_creds=$(aws secretsmanager get-secret-value --secret-id ${AWS::StackName}-dockerhub/creds --query 'SecretString' | sed 's/\"//g' )
                - dh_username=$(echo $dh_creds | cut -f1 -d' ')
                - echo $dh_creds | cut -f2 -d' ' > dh.pass
                - cat dh.pass | docker login -u $dh_username --password-stdin
            build:
              commands:
                - export HUB_IMAGE_TAG=$(date +"%F-%H-%M-%S")
                - docker build -t ${HubImageName}:$HUB_IMAGE_TAG -f hub/dockerfile hub/
                - docker push ${HubImageName}:$HUB_IMAGE_TAG
                - aws eks update-kubeconfig --name ${Cluster}
                - sed -i "s|INSTANCE_ROLE_ARN|${NodeInstanceRole.Arn}|" configs/aws-auth-cm.yaml
                - sed -i "s|USER_ACCESS_ROLE_ARN|${UserAccessRole.Arn}|" configs/aws-auth-cm.yaml
                - kubectl apply -f configs/aws-auth-cm.yaml
                - kubectl apply -f configs/ebs-sc.yaml
                - export HELM_HOST=127.0.0.1:44134
                - eksctl utils associate-iam-oidc-provider --cluster ${Cluster} --approve
                - python3 scripts/service_account_role.py --cluster-name ${Cluster}
                - helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
                - helm repo update
                - helm upgrade jupyter jupyterhub/jupyterhub
                    --install
                    --namespace jupyter
                    --version 0.9.1
                    --values helm_config.yaml
                    --timeout=6m0s
                    --atomic
                    --set auth.admin.users[0]=${AdminUserName}
                    --set proxy.secretToken=$(openssl rand -hex 32),proxy.service.nodePorts.http='${NodeProxyPort}'                    
                    --set custom.IMAGE_NAME='${ImageName}',custom.IMAGE_TAG='${ImageTag}'
                    --set custom.OAUTH_JUPYTER_URL=$( [ -n '${JupyterHubURL}' ] && echo '${JupyterHubURL}' || echo 'https://${LoadBalancer.DNSName}' )
                    --set custom.OAUTH_DNS_NAME='${OAuthDnsName}'
                    --set custom.CLUSTER_NAME=${Cluster},custom.AZ_NAME=${AWS::Region}d
                    --set hub.image.tag=$HUB_IMAGE_TAG,hub.image.name=${HubImageName}
                - kubectl -n jupyter rollout status -w deployment.apps/hub
                - kubectl --logtostderr=true -n jupyter patch deployment hub --patch "$(cat configs/deploy-patch.yaml)" || true
                - kubectl create clusterrolebinding cluster-pv --clusterrole=system:persistent-volume-provisioner --serviceaccount=jupyter:hub --dry-run -o yaml | kubectl apply -f -
                - curl -o aws-k8s-cni.yaml https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.6/config/v1.6/aws-k8s-cni.yaml
                - sed -i -e 's/us-west-2/${AWS::Region}/' aws-k8s-cni.yaml
                - kubectl apply -f aws-k8s-cni.yaml
                - helm repo add stable https://kubernetes-charts.storage.googleapis.com
                - helm repo update
                - helm fetch stable/cluster-autoscaler
                - tar -zxf cluster-autoscaler*
                - helm upgrade autoscaler stable/cluster-autoscaler
                    --install
                    --namespace autoscaler
                    --values cluster-autoscaler*/values.yaml
                    --atomic
                    --timeout=2m0s
                    --set autoDiscovery.clusterName=${Cluster},awsRegion=${AWS::Region},sslCertPath='/etc/kubernetes/pki/ca.crt',rbac.create=True,nodeSelector."hub\\.jupyter\\.org/node-purpose"=core
                - curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/master/k8s-yaml-templates/quickstart/cwagent-fluentd-quickstart.yaml | sed "s/{{cluster_name}}/${Cluster}/;s/{{region_name}}/${AWS::Region}/" | kubectl apply -f -
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref ProjectLogGroup
  

# This helm config file modifies the defaults found in zero-to-jupyterhub-k8s/jupyterhub/
# Possible values are scattered throughout the doc starting at https://z2jh.jupyter.org/en/latest/setup-jupyterhub.html

# Proxy token can be recreated via `openssl rand -hex 32`. This token isn't really used anywhere else so I'm not sure of the security concerns.
proxy:
  secretToken: <>
auth:
  admin:
    users:
      - emlundell

  whitelist:
    users:
      - emlundell

singleuser:
  image:
    name: 553778890976.dkr.ecr.us-east-1.amazonaws.com/asf-franz-labs
    tag: build.16
  extraEnv:
    # Keys needed for the notebook servers to talk to s3
    AWS_ACCESS_KEY_ID: <>
    AWS_SECRET_ACCESS_KEY: <>
  lifecycleHooks:
    postStart:
      exec:
        # When the jupyterhub server mounts the EBS volumes to $HOME, it "deletes" anything in that directory.
        # The volumes cannot be mounted anywhere else.
        # Thus hidden directories for condas and other programs when originally built will get destroyed.
        # This hook takes copies of those files (safely moved during the image build) and copies them back to $HOME.
        command: ["gitpuller", "https://github.com/asfadmin/asf-jupyter-notebooks.git", "eml", "notebooks"]
  #profileList:
  #  - display_name: "Small: default"
  #    description: |
  #      A small job. 1 CPU, no GPU. This is the default.
  #    default: True
  #    kubespawner_override:
  #      cpu_limit: 2
  #      cpu_guarantee: 1
  #      extra_resource_limits: {}
  #  - display_name: "Big: 8 CPUs"
  #    description: |
  #      A big job. 8 CPUs, no GPU, and 64GB of RAM
  #    kubespawner_override:
  #      cpu_limit: 1
  #      cpu_guarantee: 1
  #      mem_limit: "2G"
  #      mem_guaranttee: "1G"
  #      extra_resource_limits: {}

# Always pull the latest image when the Helm chart is updated
prePuller:
  continuous:
    enabled: true
  hook:
    enabled: true

#proxy:
#  https:
#    enabled: true
#    type: offload
#  service:
#    annotations:
#      # Certificate ARN
#      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:us-east-1:553778890976:certificate/862ecb20-8df6-458a-b45d-bc03b9b02af5"
#      # The protocol to use on the backend, we use TCP since we're using websockets
#      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "tcp"
#      # Which ports should use SSL
#      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
#      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "3600"

scheduling:
    userPods:
        nodeAffinity:
            # matchNodePurpose valid options:
            # - ignore
            # - prefer (the default)
            # - require
        matchNodePurpose: require
    userScheduler:
        enabled: true
